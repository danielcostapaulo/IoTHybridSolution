{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has most of the code done in python for this project. For the sake of documentation, there was an attempt to delete the minimum amount of code possible, so, consequentialy, some sections have poor results.The structure of this file follows the \"chronological\" order of this work, which results in the code and commentaries feeling incoherent at times and the overall objective hard to follow, however, it also shows the evolution of the work and some of the story behind this project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first major part of this project is to study, and analize the potential of the LSM6DSOX IMU. This IMU has many interesting features and usefull functions that i will not refer here, however, it was important to study the MLC capabilities of this accelerometer since it can provide a great low power solution to the classification problem. This study was done in 3 steps:\n",
    "\n",
    "1-Analize raw data gathered from the accelerometer.\n",
    "\n",
    "2-Analize the features utilized in the internal MLC.\n",
    "\n",
    "3-Analize results gathered from the internal MLC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This first snipit of code has the necessary includes and some function definitions for later use. Most of the code in this file is independent,however, most sections depend on this one, therefore it is important to always run this code before executing any other section.\n",
    "\n",
    "P2P stands for peak to peak. This function does the subtraction between the maximum and minimum of the signal.\n",
    "\n",
    "Var stand for variance. This function does the following equation: $ (\\frac{\\sum_{k=0}^{WL-2} I_{k}^{2}}{WL}) - (\\frac{\\sum_{k=0}^{WL-2} I_{k}}{WL})^{2} $\n",
    "\n",
    "Energy is self explanatory and is applies the following equation: $ \\sum_{k=0}^{WL-2} I_{k}^{2} $\n",
    "\n",
    "All of these functions were taken from the official application note:\n",
    "https://www.st.com/content/ccc/resource/technical/document/application_note/group2/5f/d8/0a/fe/04/f0/4c/b8/DM00563460/files/DM00563460.pdf/jcr:content/translations/en.DM00563460.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial as serial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import scipy.signal\n",
    "import plotly.express as ex\n",
    "from numpy.fft import fft,fftfreq\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import librosa\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def p2p(data,window_size):\n",
    "    result=[]\n",
    "    i=0\n",
    "    data_window=[]\n",
    "    for x in data:\n",
    "        i+=2\n",
    "        data_window.append(x/1000)\n",
    "        if i>=window_size:\n",
    "            i=0\n",
    "            result.append(max(data_window)-min(data_window))\n",
    "            data_window=[]\n",
    "    return result\n",
    "\n",
    "def var(data,window_size):\n",
    "    result=[]\n",
    "    i=0\n",
    "    data_window=[]\n",
    "    data_window_2=[]\n",
    "    for x in data:\n",
    "        i+=1\n",
    "        data_window.append(x/1000)\n",
    "        data_window_2.append(x/1000*x/1000)\n",
    "        if i>=window_size:\n",
    "            i=0\n",
    "            second=(sum(data_window)/window_size)\n",
    "            result.append((sum(data_window_2)/window_size)-(second*second))\n",
    "            data_window=[]\n",
    "            data_window_2=[]\n",
    "    return result\n",
    "\n",
    "def energy(data,window_size):\n",
    "    result=[]\n",
    "    i=0\n",
    "    data_window=[]\n",
    "    for x in data:\n",
    "        i+=1\n",
    "        data_window.append(x/1000*x/1000)\n",
    "        if i>=window_size:\n",
    "            i=0\n",
    "            result.append(sum(data_window))\n",
    "            data_window=[]\n",
    "    return result\n",
    "\n",
    "def process_raw(raw_lines):\n",
    "    result=[]\n",
    "    med_x=0\n",
    "    med_y=0\n",
    "    med_z=0\n",
    "    for line in raw_lines:\n",
    "        str_list=list(line.strip().split(\" \"))\n",
    "        try:\n",
    "            float_list=[float(x) for x in str_list]\n",
    "            med_x+=float_list[0]\n",
    "            med_y+=float_list[1]\n",
    "            med_z+=float_list[2]\n",
    "            result.append(float_list)\n",
    "        except:\n",
    "            continue\n",
    "    med_x=med_x/len(result)\n",
    "    med_y=med_y/len(result)\n",
    "    med_z=med_z/len(result)\n",
    "    return result,med_x,med_y,med_z\n",
    "\n",
    "\n",
    "def process_features(raw_lines,n_of_samples):\n",
    "    peak_amp_or_val=0\n",
    "    mean_flag=0\n",
    "    fft_or_peak=0\n",
    "    sample_idx=0\n",
    "    line_state=0\n",
    "    idx=0\n",
    "    Dataset=np.empty((3),dtype=object)\n",
    "    peaks=np.empty((3),dtype=object)\n",
    "    peaks_amp=np.empty((3),dtype=object)\n",
    "    mean_std=np.empty((3),dtype=object)\n",
    "    for line in raw_lines:\n",
    "        if(line==\"y:  \\n\"):\n",
    "            fft_or_peak=0\n",
    "            sample_idx=0\n",
    "            line_state=1\n",
    "            idx=0\n",
    "        elif(line==\"z:  \\n\"):\n",
    "            fft_or_peak=0\n",
    "            sample_idx=0\n",
    "            line_state=2\n",
    "            idx=0\n",
    "        str_list=list(line.strip().split(\" \"))\n",
    "        if(sample_idx!=n_of_samples):\n",
    "            try:\n",
    "                float_list=[float(x) for x in str_list]\n",
    "                if(idx==0):\n",
    "                    Dataset[line_state]=[]\n",
    "                    peaks[line_state]=[]\n",
    "                    peaks_amp[line_state]=[]\n",
    "                    mean_std[line_state]=[]\n",
    "                    idx=1\n",
    "                if(fft_or_peak==0):\n",
    "                    Dataset[line_state].append(float_list)\n",
    "                    fft_or_peak=1\n",
    "                else:\n",
    "                    if(peak_amp_or_val==0):\n",
    "                        peaks[line_state].append(float_list)\n",
    "                        peak_amp_or_val=1\n",
    "                    else:\n",
    "                        if mean_flag==0:\n",
    "                            peaks_amp[line_state].append(float_list)\n",
    "                            mean_flag=1\n",
    "                        else:\n",
    "                            mean_std[line_state].append(float_list)\n",
    "                            fft_or_peak=0\n",
    "                            peak_amp_or_val=0\n",
    "                            mean_flag=0   \n",
    "                            sample_idx+=1\n",
    "            except:\n",
    "                continue\n",
    "    return Dataset,peaks,peaks_amp,mean_std\n",
    "\n",
    "    \n",
    "\n",
    "def build_dataset(FFTs,peak_vals,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std):\n",
    "    X_train=[]\n",
    "    X_test=[]\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    if peaks_or_fft==1:\n",
    "        for class_n,all_samples_in_class in enumerate(peak_vals):\n",
    "            n_of_samples=len(peak_vals[class_n][0])\n",
    "            train_samples=int(n_of_samples*(1-dataset_ratio))\n",
    "            for sample_n in range(len(all_samples_in_class[0])):\n",
    "                sample=[]\n",
    "                for axis in range(3):\n",
    "                    axis_sample=[]\n",
    "                    axis_amp=[]\n",
    "                    for n_of_peaks in range(len(all_samples_in_class[0][0])):\n",
    "                        axis_sample.append(all_samples_in_class[axis][sample_n][n_of_peaks])\n",
    "                        axis_amp.append(peak_amp[class_n][axis][sample_n][n_of_peaks])\n",
    "                    max_peak=axis_amp[0]\n",
    "                    if sort==1:\n",
    "                        sort_minus_one=0\n",
    "                        c = []\n",
    "                        for i in range(len(axis_sample)):\n",
    "                            if axis_sample[i]==-1:\n",
    "                                sort_minus_one+=1\n",
    "                            else:\n",
    "                                c.append((axis_sample[i], axis_amp[i]))\n",
    "                        r = sorted(c)\n",
    "                        for i in range(len(r)):\n",
    "                            axis_sample[i], axis_amp[i] = r[i]\n",
    "                        for i in range(sort_minus_one):\n",
    "                            axis_sample.append(-1)\n",
    "                            axis_amp.append(0)\n",
    "                    for peak in range(n_of_peaks_to_use):\n",
    "                        sample.append(axis_sample[peak])\n",
    "                        if use_of_amps==1:\n",
    "                            sample.append(axis_amp[peak])\n",
    "                    if add_energy==1:\n",
    "                        sum_of_fft=0\n",
    "                        for val in FFTs[class_n][axis][sample_n]:\n",
    "                            sum_of_fft+=val\n",
    "                        sample.append(sum_of_fft/len(FFTs[class_n][axis][sample_n]))\n",
    "                    if add_mean==1:\n",
    "                        sample.append(mean_std[class_n][axis][sample_n][0])\n",
    "                    if add_std==1:\n",
    "                        sample.append(mean_std[class_n][axis][sample_n][1])\n",
    "                if(sample_n<train_samples):\n",
    "                    X_train.append(sample)\n",
    "                    y_train.append(class_n)\n",
    "                else:\n",
    "                    X_test.append(sample)\n",
    "                    y_test.append(class_n)\n",
    "    elif peaks_or_fft==0:\n",
    "\n",
    "        for class_n,all_samples_in_class in  enumerate(FFTs):\n",
    "            n_of_samples=len(FFTs[0][0])\n",
    "            train_samples=int(n_of_samples*(1-dataset_ratio))\n",
    "            for sample_n in range(len(all_samples_in_class[1])):\n",
    "                sample=[]\n",
    "                for axis in  range(3):\n",
    "                    aux=all_samples_in_class[axis][sample_n][:len(all_samples_in_class[axis][sample_n])]\n",
    "                    sample.extend(aux)\n",
    "                if(sample_n<train_samples):\n",
    "                    X_train.append(sample)\n",
    "                    y_train.append(class_n)\n",
    "                else:\n",
    "                    X_test.append(sample)\n",
    "                    y_test.append(class_n)\n",
    "    return X_train,y_train,X_test,y_test\n",
    "def port_kmeans_to_C(kmeans,file_name,X_train,quantize):\n",
    "    #first it will determine the maximum distance of each inner cluster, that is, the distance of the furtherst training sample of each cluster\n",
    "    centroid_list=kmeans.cluster_centers_\n",
    "    max_dist=[0]*len(centroid_list)\n",
    "    for sample_idx,sample in enumerate(X_train):\n",
    "        min_dist=0\n",
    "        min_aux=1\n",
    "        assigned_centroid=0\n",
    "        for centroid_idx,centroid in enumerate(centroid_list):\n",
    "            distance=0\n",
    "            for feature in range(len(sample)):\n",
    "                distance+=np.sqrt((sample[feature]-centroid[feature])**2)\n",
    "            if min_aux==1:\n",
    "                min_dist=distance\n",
    "                assigned_centroid=centroid_idx\n",
    "                min_aux=0\n",
    "            elif distance<min_dist:\n",
    "                min_dist=distance\n",
    "                assigned_centroid=centroid_idx\n",
    "        if min_dist>max_dist[assigned_centroid]:\n",
    "            max_dist[assigned_centroid]=min_dist\n",
    "    #this will now make the code, in which the template is constant but the values are need to be filled\n",
    "    max_dist_str=\"\"\n",
    "    cluster_centers=\"\"\n",
    "    #quantization is simply turning every value into int, the logic is the same\n",
    "    if quantize==1:\n",
    "        #first the max_distances\n",
    "        max_dist_str=\"int max_dist[\"+str(len(max_dist))+\"]={\"+str(int(max_dist[0]))\n",
    "        for val_idx,val in enumerate(max_dist):\n",
    "            if val_idx==0:\n",
    "                continue\n",
    "            max_dist_str+=\",\"+str(int(val))\n",
    "        max_dist_str+=\"};\"\n",
    "        #now the cluster centers\n",
    "        cluster_centers=\"int clusters[\"+str(len(max_dist))+\"][\"+str(len(X_train[0]))+\"]={\"\n",
    "        for cluster_idx,cluster in enumerate(kmeans.cluster_centers_):\n",
    "            cluster_centers+=\"{\"+str(int(cluster[0]))\n",
    "            for val_idx,val in enumerate(cluster):\n",
    "                if val_idx==0:\n",
    "                    continue\n",
    "                else:\n",
    "                    cluster_centers+=\",\"+str(int(val))\n",
    "            if cluster_idx!=len(kmeans.cluster_centers_)-1:\n",
    "                cluster_centers+=\"},\"\n",
    "        cluster_centers+=\"}};\"\n",
    "    else:\n",
    "        max_dist_str=\"float max_dist[\"+str(len(max_dist))+\"]={\"+str(max_dist[0])\n",
    "        for val in max_dist:\n",
    "            if val==max_dist[0]:\n",
    "                continue\n",
    "            max_dist_str+=\",\"+str(val)\n",
    "        max_dist_str+=\"};\"\n",
    "        cluster_centers=\"float clusters[\"+str(len(max_dist))+\"][\"+str(len(X_train[0]))+\"]={\"\n",
    "        for cluster_idx,cluster in enumerate(kmeans.cluster_centers_):\n",
    "            cluster_centers+=\"{\"+str(cluster[0])\n",
    "            for val_idx,val in enumerate(cluster):\n",
    "                if val_idx==0:\n",
    "                    continue\n",
    "                else:\n",
    "                    cluster_centers+=\",\"+str(val)\n",
    "            if cluster_idx!=len(kmeans.cluster_centers_)-1:\n",
    "                cluster_centers+=\"},\"\n",
    "        cluster_centers+=\"}};\"\n",
    "    code_template='''\\\n",
    "    #include <iostream>\n",
    "    #include <cmath>\n",
    "    using namespace std;\n",
    "\n",
    "    class KMeans{\n",
    "        public:\n",
    "            %s\n",
    "            %s\n",
    "            int predict(float* sample);\n",
    "    };\n",
    "    int KMeans::predict(float * sample){\n",
    "        float min_distance=0;\n",
    "        bool min_aux=1;\n",
    "        int res=0;\n",
    "        for(int i=0;i<int(%s);i++){\n",
    "            float distance=0;\n",
    "            for(int j=0;j<int(%s);j++){\n",
    "                distance=distance+sqrt((sample[j]-KMeans::clusters[i][j])*(sample[j]-KMeans::clusters[i][j]));\n",
    "            }\n",
    "            cout<<distance<<endl;\n",
    "            if(min_aux==1){\n",
    "                min_distance=distance;\n",
    "                min_aux=0;\n",
    "                res=i;\n",
    "            }\n",
    "            else{\n",
    "                if(min_distance>distance){\n",
    "                    min_distance=distance;\n",
    "                    res=i;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        if(min_distance>max_dist[res]){\n",
    "            return -1;\n",
    "        }\n",
    "        return res;\n",
    "    }\n",
    "    '''\n",
    "    c_code=code_template % (max_dist_str,cluster_centers,len(max_dist),len(X_train[0]))\n",
    "    with open(file_name+\".h\",'w') as file:\n",
    "        file.write(c_code)\n",
    "\n",
    "\n",
    "def port_scaller(scaller,file_name,quantize):\n",
    "    mean_vals=scaller.mean_\n",
    "    var_vals=scaller.var_\n",
    "    var_str=\"\"\n",
    "    mean_str=\"\"\n",
    "    if quantize==1:\n",
    "        mean_str=\"int means[\"+str(len(mean_vals))+\"]={\"+str(int(mean_vals[0]))\n",
    "        var_str=\"int vars[\"+str(len(var_vals))+\"]={\"+str(int(math.sqrt(var_vals[0])))\n",
    "        for val_idx in range(len(mean_vals)):\n",
    "            if val_idx!=0:\n",
    "                mean_str+=\",\"+str(int(mean_vals[val_idx]))\n",
    "                var_str+=\",\"+str(int(math.sqrt(var_vals[val_idx])))\n",
    "        mean_str+=\"};\"\n",
    "        var_str+=\"};\"\n",
    "    else:\n",
    "        mean_str=\"int means[\"+str(len(mean_vals))+\"]={\"+str(mean_vals[0])\n",
    "        var_str=\"int vars[\"+str(len(var_vals))+\"]={\"+str(math.sqrt(var_vals[0]))\n",
    "        for val_idx in range(len(mean_vals)):\n",
    "            if val_idx!=0:\n",
    "                mean_str+=\",\"+str(mean_vals[val_idx])\n",
    "                var_str+=\",\"+str(math.sqrt(var_vals[val_idx]))\n",
    "        mean_str+=\"};\"\n",
    "        var_str+=\"};\"\n",
    "    code_template='''\\\n",
    "    #include <iostream>\n",
    "\n",
    "    class Scaller{\n",
    "        public:\n",
    "            %s\n",
    "            %s\n",
    "            void transform(float* sample,float* dest);      \n",
    "    };\n",
    "\n",
    "    void Scaller::transform(float* sample,float* dest){\n",
    "        float new_sample[21]={0};\n",
    "        for(int i=0;i<21;i++){\n",
    "            new_sample[i]=(sample[i]-means[i])/vars[i];\n",
    "        }\n",
    "        memcpy(dest, new_sample, sizeof(float) * 21);\n",
    "\n",
    "    }\n",
    "    '''\n",
    "    c_code=code_template % (mean_str,var_str)\n",
    "    with open(file_name+\".h\",'w') as file:\n",
    "        file.write(c_code)\n",
    "\n",
    "    \n",
    "    \n",
    "init_dir=os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code turns the seperate axis into a joint axis. There are some inputs made by the user to either filter throught a high pass filter ot take of the averages calculated in the previous snipit. It is important to note that, if the user utilized the HP filter, it will print the coeficients utilized. These coeficients can be utilized in the MLC. The result is ploted.\n",
    "\n",
    "The MLC selectable features is calculated next and its respective results are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\motor')\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import plotly.express as px\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "file_idle=open(\"idle_motor.txt\",'r')\n",
    "file_shake=open(\"shake_motor.txt\",'r')\n",
    "#os.chdir(r'C:\\Users\\danip\\OneDrive\\Desktop\\IST-vibration\\Data\\coffe_dataset_v1')\n",
    "#file_idle=open(\"data0.txt\",'r')\n",
    "#file_shake=open(\"data1.txt\",'r')\n",
    "idle_data_raw=file_idle.readlines()\n",
    "shake_data_raw=file_shake.readlines()\n",
    "idle_float,med_idle_x,med_idle_y,med_idle_z=process_raw(idle_data_raw)\n",
    "shake_float,med_shake_x,med_shake_y,med_shake_z=process_raw(shake_data_raw)\n",
    "idle_V=[]\n",
    "shake_V=[]\n",
    "idle_float_offset=[]\n",
    "shake_float_offset=[]\n",
    "filter_mode=input(\"Filter signals?(y/n):\")\n",
    "if filter_mode=='y':\n",
    "    filter_frequency=float(input(\"Enter filter frequency:\"))\n",
    "    for line in idle_float:\n",
    "        idle_V.append(math.sqrt(line[0]*line[0]+line[1]*line[1]+line[2]*line[2]))\n",
    "    for line in shake_float:\n",
    "        shake_V.append(math.sqrt(line[0]*line[0]+line[1]*line[1]+line[2]*line[2]))\n",
    "    b,a=scipy.signal.butter(1,2*filter_frequency/26,btype='high')\n",
    "    print(\"a:\",a)\n",
    "    print(\"b:\",b)\n",
    "    idle_V=scipy.signal.filtfilt(b,a,idle_V)\n",
    "    shake_V=scipy.signal.filtfilt(b,a,shake_V)\n",
    "if filter_mode=='n':\n",
    "    offset_mode=input(\"Take off indle offset?(y/n):\")\n",
    "    if offset_mode=='y':\n",
    "        for line in idle_float:\n",
    "            new_line=[line[0]-med_idle_x,line[1]-med_idle_y,line[2]-med_idle_z]\n",
    "            idle_float_offset.append(new_line)\n",
    "        for line in shake_float:\n",
    "            new_line=[line[0]-med_idle_x,line[1]-med_idle_y,line[2]-med_idle_z]\n",
    "            shake_float_offset.append(new_line)\n",
    "        with open(\"offset_idle.txt\",'w') as idle_file:\n",
    "            for line in idle_float:\n",
    "                idle_file.write(\" \".join(str(numb) for numb in line if abs(numb)>1))\n",
    "                idle_file.write('\\n')\n",
    "        with open(\"offset_shake.txt\",'w') as idle_file:\n",
    "            for line in shake_float:\n",
    "                idle_file.write(\" \".join(str(numb) for numb in line if abs(numb)>1))\n",
    "                idle_file.write('\\n')\n",
    "        for line in idle_float_offset:\n",
    "            idle_V.append(math.sqrt(line[0]*line[0]+line[1]*line[1]+line[2]*line[2]))\n",
    "        for line in shake_float_offset:\n",
    "            shake_V.append(math.sqrt(line[0]*line[0]+line[1]*line[1]+line[2]*line[2]))\n",
    "    else:\n",
    "        for line in idle_float:\n",
    "            idle_V.append(math.sqrt(line[0]*line[0]+line[1]*line[1]+line[2]*line[2]))\n",
    "        for line in shake_float:\n",
    "            shake_V.append(math.sqrt(line[0]*line[0]+line[1]*line[1]+line[2]*line[2]))\n",
    "plt.plot(shake_V,label=\"shake\")\n",
    "plt.plot(idle_V,label=\"idle\")\n",
    "plt.title(\"Data\")\n",
    "plt.xlabel(\"Sample number\")\n",
    "plt.ylabel(\"mg\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#features section\n",
    "\n",
    "window_size=int(input(\"Enter window size:\"))\n",
    "user_mode=input(\"Enter desired feature(P,V,E):\")\n",
    "if user_mode=='E':\n",
    "    shake_features=energy(shake_V,window_size)\n",
    "    idle_features=energy(idle_V,window_size)\n",
    "elif user_mode=='V':\n",
    "    shake_features=var(shake_V,window_size)\n",
    "    idle_features=var(idle_V,window_size)\n",
    "else:\n",
    "    shake_features=p2p(shake_V,window_size)\n",
    "    idle_features=p2p(idle_V,window_size)\n",
    "plt.figure()\n",
    "plt.plot(shake_features,label=\"shake\")\n",
    "plt.plot(idle_features,label=\"idle\")\n",
    "plt.legend()\n",
    "plt.title(\"Variance plot\")\n",
    "plt.xlabel(\"Sample number\")\n",
    "plt.ylabel(\"Variance value\")\n",
    "plt.figure()\n",
    "plt.hist(shake_features,label='shake')\n",
    "plt.hist(idle_features,label='idle')\n",
    "plt.title(\"Variance histogram\")\n",
    "plt.xlabel(\"Variance value\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "for i in range(len(idle_features)):\n",
    "    if i<int(len(idle_features)*0.66):\n",
    "        X_train.append(idle_features[i])\n",
    "        y_train.append(0)\n",
    "    else:\n",
    "        X_test.append(idle_features[i])\n",
    "        y_test.append(0)\n",
    "for i in range(len(shake_features)):\n",
    "    if i<int(len(shake_features)*0.66):\n",
    "        X_train.append(shake_features[i])\n",
    "        y_train.append(1)\n",
    "    else:\n",
    "        X_test.append(shake_features[i])\n",
    "        y_test.append(1)\n",
    "\n",
    "X_train=np.array(X_train).reshape(-1,1)\n",
    "X_test=np.array(X_test).reshape(-1,1)\n",
    "\n",
    "clf=DecisionTreeClassifier(random_state=0,max_depth=1)\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "clf.fit(X_train,y_train)\n",
    "test_labels=clf.predict(X_test)\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "#clf.tree_.threshold[0]=0.0001\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "for idx in range(len(y_test)):\n",
    "    if y_test[idx]==1 and test_labels[idx]==1:\n",
    "        TP+=1\n",
    "    elif y_test[idx]==0 and test_labels[idx]==1:\n",
    "        FP+=1\n",
    "    elif y_test[idx]==0 and test_labels[idx]==0:\n",
    "        TN+=1\n",
    "    elif y_test[idx]==1 and test_labels[idx]==0:\n",
    "        FN+=1\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+TN+FP+FN))\n",
    "\n",
    "print(\"Precision:\",TP/(TP+FP))\n",
    "print(\"Recall:\",TP/(TP+FN))\n",
    "p=TP/(TP+FP)\n",
    "r=TP/(TP+FN)\n",
    "print(\"F1-score\",2*(p*r)/(p+r))\n",
    "disp.plot()\n",
    "plt.show()\n",
    "plot_tree(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is to analize result files in which there are 2 additional values in lines: the predicted result and ground truth. Most of the logic is identical to the previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\results')\n",
    "file_result=open(\"motor_motor_EF3.txt\",'r')\n",
    "#file_result=open(\"motor_to_newMotorFEM3_1.txt\",'r')\n",
    "#file_result=open(\"motor_to_newMotor_FEM3.txt\",'r')\n",
    "#file_result=open(\"data0.txt\",'r')\n",
    "result_data_raw=file_result.readlines()\n",
    "result_float,med_res_x,med_res_y,med_res_z=process_raw(result_data_raw)\n",
    "time=[x/26 for x in range(len(result_float))]\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "GT=[]\n",
    "pred=[]\n",
    "print(\"          SHAKE     IDLE\")\n",
    "for line in result_float:\n",
    "    GT.append(line[4])\n",
    "    pred.append(line[3])\n",
    "    if line[3]==1 and line[4]==1:\n",
    "        TP+=1\n",
    "    elif line[3]==1 and line[4]==0:\n",
    "        FP+=1\n",
    "    elif line[3]==0 and line[4]==0:\n",
    "        TN+=1\n",
    "    elif line[3]==0 and line[4]==1:\n",
    "        FN+=1\n",
    "print(\"SHAKE     \",TP,\"     \",FN)\n",
    "print(\"IDLE      \",FP,\"     \",TN)\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+TN+FP+FN))\n",
    "\n",
    "p=TP/(TP+FP)\n",
    "r=TP/(TP+FN)\n",
    "print(\"Precision:\",TP/(TP+FP))\n",
    "print(\"Recall:\",TP/(TP+FN))\n",
    "print(\"F1-score\",2*(p*r)/(p+r))\n",
    "result_V=[]\n",
    "for line in result_float:\n",
    "    x=line[0]/2\n",
    "    y=line[1]/2\n",
    "    z=line[2]/2\n",
    "    result_V.append(math.sqrt(x*x+y*y+z*z))\n",
    "filter_mode=input(\"Filter signals?(y/n):\")\n",
    "if filter_mode=='y':\n",
    "    filter_frequency=float(input(\"Enter filter frequency:\"))\n",
    "    b,a=scipy.signal.butter(1,2*filter_frequency/26,btype='high')\n",
    "    print(\"a:\",a)\n",
    "    print(\"b:\",b)\n",
    "    result_V=scipy.signal.filtfilt(b,a,result_V)\n",
    "    for idx,val in enumerate(result_V):\n",
    "        if val>10:\n",
    "            result_V[idx]=10\n",
    "        if val<-10:\n",
    "            result_V[idx]=-10\n",
    "\n",
    "\n",
    "\n",
    "pred_rt=[]\n",
    "buffer=[0,0,0,0,0,0,0,0,0,0]\n",
    "buffer_idx=0\n",
    "meta_classifier_idx=0\n",
    "curr_state=0\n",
    "\n",
    "result_features=energy(result_V,10)\n",
    "for idx,val in enumerate(result_V):\n",
    "    if idx>160:\n",
    "        buffer[buffer_idx]=val\n",
    "        en=0\n",
    "        if buffer_idx!=9:\n",
    "            buffer_idx+=1\n",
    "        else:\n",
    "            buffer_idx=0\n",
    "            for samp in buffer:\n",
    "                en+=(samp/1000)*(samp/1000)\n",
    "            prediction=clf.predict(np.array(result_features[int((idx-160)/10)]).reshape(1,-1))[0]\n",
    "            if prediction!=curr_state:\n",
    "                if meta_classifier_idx==5:\n",
    "                    curr_state=prediction\n",
    "                    meta_classifier_idx=0\n",
    "                else:\n",
    "                    meta_classifier_idx+=1\n",
    "            else:\n",
    "                if meta_classifier_idx!=0:\n",
    "                    meta_classifier_idx-=1\n",
    "    pred_rt.append(curr_state)\n",
    "\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "for idx in range(len(pred_rt)):\n",
    "    if pred_rt[idx]==1 and GT[idx]==1:\n",
    "        TP+=1\n",
    "    elif pred_rt[idx]==1 and GT[idx]==0:\n",
    "        FP+=1\n",
    "    elif pred_rt[idx]==0 and GT[idx]==0:\n",
    "        TN+=1\n",
    "    elif pred_rt[idx]==0 and GT[idx]==1:\n",
    "        FN+=1\n",
    "print(\"Accuracy:\",(TP+TN)/(TP+TN+FP+FN))\n",
    "\n",
    "p=TP/(TP+FP)\n",
    "r=TP/(TP+FN)\n",
    "print(\"Precision:\",TP/(TP+FP))\n",
    "print(\"Recall:\",TP/(TP+FN))\n",
    "print(\"F1-score\",2*(p*r)/(p+r))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "GT=[]\n",
    "Pred=[]\n",
    "delay=[]\n",
    "prev_GT=result_float[0][4]\n",
    "delay_flag=0\n",
    "delay_init=0\n",
    "for index,line in enumerate(result_float):\n",
    "    GT.append(line[4])\n",
    "    Pred.append(line[3])\n",
    "    if line[4]!=prev_GT:\n",
    "        delay_flag=1\n",
    "        delay_init=index\n",
    "    prev_GT=line[4]\n",
    "    if delay_flag==1:\n",
    "        if line[3]==prev_GT:\n",
    "            delay.append((index-delay_init)/26)\n",
    "            delay_flag=0\n",
    "print(\"delays:\",delay)\n",
    "\n",
    "print(pred_rt)\n",
    "print(GT)\n",
    "cm=confusion_matrix(GT,pred_rt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "window_size=10\n",
    "filter_feautures=input(\"Filter features?(y/n):\")\n",
    "if filter_feautures=='y':\n",
    "    result_features=[i for i in result_features if i<2000]\n",
    "\n",
    "df=pd.DataFrame(dict(Raw_data=result_V,Sample_n=range(len(result_V)),Time=time))\n",
    "fig=ex.line(df,x=\"Time\",y=\"Raw_data\",title=\"Raw data\")\n",
    "fig.update_traces(name=\"Line 1\")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time[s]\",  # X-axis label\n",
    "    yaxis_title=\"Filtered data[mg]\",  # Y-axis label\n",
    "    title=\"Filtered data obtained\",  # Title of the plot (optional)\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show(\"notebook\")\n",
    "fig1=ex.line(y=result_features)\n",
    "fig1.show(\"notebook\")\n",
    "df=pd.DataFrame(dict(GroundTruth=GT,Prediction=pred_rt))\n",
    "fig2=ex.line(df)\n",
    "fig2.show(\"notebook\")\n",
    "\n",
    "plt.figure()\n",
    "plt.figure().set_figwidth(15)\n",
    "plt.plot(time,result_V,label=\"Filtered Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Filtered data\")\n",
    "plt.xlabel(\"Time[s]\")\n",
    "plt.ylabel(\"Filtered data[mg]\")\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(result_features,label=\"Features\")\n",
    "plt.legend()\n",
    "plt.title(\"Features\")\n",
    "plt.xlabel(\"Sample number\")\n",
    "plt.ylabel(\"Energy value\")\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(time,GT,label=\"Ground Truth\")\n",
    "plt.plot(time,pred_rt,label=\"Prediction\")\n",
    "plt.legend()\n",
    "plt.title(\"MLC results\")\n",
    "plt.xlabel(\"Time[s]\")\n",
    "plt.ylabel(\"Class number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to showing and comparing the results from the ON/OFF gathered from the 3D printers experiment. Some results are the fact that it is almost 100% accurate when the 3D printer is not working. When it is working it seems to be somewhat inconsistent. This is due to the fact that:\n",
    "\n",
    "1- The algorithm was not trained especificaly for the 3D printer, therefore, in this case, it is insensative to the vibration.\n",
    "\n",
    "2- The 3D printer itself can be stationary while working which results in the algorithm detecting it as OFF while it is tecnacly working.\n",
    "\n",
    "This experiment serves to show that while the LSM6DSOX MLC is a very useful tool to detect the ON/OFF state of a machine, it should be accompanied by some sort of software to increase accuracy. Some basic and efficient solutions could a bigger meta-classifier or increase the amount of samples utilized in the MLC. In this experiment, 10 samples were used while the accelerometer was working at 26Hz with a meta-classifier of 3 samples, which means if the machine is still(or with low vibration) for (10/26)*3=1.15s it will register as an OFF, which is common for a 3D printer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\3d_printer_on_off\\Values')\n",
    "\n",
    "df_flag=1\n",
    "df=pd.DataFrame()\n",
    "files=os.listdir('.')\n",
    "numbers=[]\n",
    "final_files=[]\n",
    "for filename in files:\n",
    "    try:\n",
    "        digits=[char for char in filename if char.isdigit()]\n",
    "        numbers.append(int(''.join(num for num in digits)))\n",
    "        final_files.append(filename)\n",
    "    except:\n",
    "        print(\"File \",filename,\" does not have a number, therefor shouldent be in this folder.\")\n",
    "sorted_files=sorted(zip(numbers,files))\n",
    "numbers,files=zip(*sorted_files)\n",
    "files=list(files)\n",
    "for filename in files:\n",
    "    times=[]\n",
    "    values=[]\n",
    "    with open(filename,'r') as csvfile:\n",
    "        csvreader=csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            non_ms_row=row[0].split('.')[0]\n",
    "            if(non_ms_row!='ts'):\n",
    "                times.append(non_ms_row)\n",
    "                values.append(int(row[1]))\n",
    "        sub_data={'ts':times[::-1],'ON_OFF':values[::-1]}\n",
    "        if df_flag==1:\n",
    "            df=pd.DataFrame(sub_data)\n",
    "            df_flag=0\n",
    "        else:\n",
    "            sub_df=pd.DataFrame(sub_data)\n",
    "            df=pd.concat([df,sub_df],ignore_index=True)\n",
    "\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\3d_printer_on_off\\Ground_Truth')\n",
    "intervals=[]\n",
    "with open('Ground_Truth_ON_OFF.csv','r') as csvfile:\n",
    "    csvreader=csv.reader(csvfile)\n",
    "    print_times=[]\n",
    "    print_flag=0\n",
    "    for row in csvreader:\n",
    "        if print_flag==0 and row[1]=='Start':\n",
    "            datetime_obj=datetime.datetime.strptime(row[0],'%m/%d/%Y %H:%M')\n",
    "            print_times.append(datetime_obj)\n",
    "            print_flag=1\n",
    "        elif print_flag==1 and row[1]=='Stop':\n",
    "            datetime_obj=datetime.datetime.strptime(row[0],'%m/%d/%Y %H:%M')\n",
    "            print_times.append(datetime_obj)\n",
    "            print_flag=0\n",
    "            intervals.append(print_times)\n",
    "            print_times=[]\n",
    "        else:\n",
    "            print(\"Something is wrong\")\n",
    "err=0\n",
    "corr=0\n",
    "ts=[]\n",
    "pred=[]\n",
    "GT=[]\n",
    "for index,df_row in df.iterrows():\n",
    "    if df_row.iloc[0]!='ts':\n",
    "        pred.append(df_row.iloc[1])\n",
    "        registered_time=datetime.datetime.strptime(df_row.iloc[0],'%Y-%m-%d %H:%M:%S')\n",
    "        ts.append(registered_time)\n",
    "        prev_time=datetime.datetime(2024,7,11,1,10,10)\n",
    "        time_flag=0\n",
    "        for print_time in intervals:\n",
    "            if registered_time<print_time[0] and registered_time<prev_time:\n",
    "                GT.append(0)\n",
    "                if df_row.iloc[1]==0:\n",
    "                    corr+=1\n",
    "                else:\n",
    "                    err+=1\n",
    "                time_flag=1\n",
    "                break\n",
    "            if registered_time>print_time[0] and registered_time<print_time[1]:\n",
    "                GT.append(1)\n",
    "                if df_row.iloc[1]==1:\n",
    "                    corr+=1\n",
    "                else:\n",
    "                    err+=1\n",
    "                time_flag=1\n",
    "                break\n",
    "            prev_time=print_time[1]\n",
    "        if time_flag==0:\n",
    "            GT.append(0)\n",
    "            if df_row.iloc[1]==0:\n",
    "                corr+=1\n",
    "            else:\n",
    "                err+=1\n",
    "print(\"Accuracy:\",corr/(corr+err)*100,\"%\")\n",
    "cm=confusion_matrix(GT,pred)\n",
    "print(cm)\n",
    "classes=[\"OFF\",\"ON\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=classes)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=ts, y=pred,\n",
    "                    mode='lines',\n",
    "                    name='Predicted values'))\n",
    "fig.add_trace(go.Scatter(x=ts, y=GT,\n",
    "                    mode='lines',\n",
    "                    name='Ground Truth'))\n",
    "fig.show()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(ts,GT,label=\"Ground Truth\")\n",
    "plt.plot(ts,pred,label=\"Prediction\")\n",
    "plt.legend()\n",
    "plt.title(\"MLC results\")\n",
    "plt.xlabel(\"Time[s]\")\n",
    "plt.ylabel(\"Class number\")\n",
    "plt.show()\n",
    "TP=cm[1][1]\n",
    "TN=cm[0][0]\n",
    "FP=cm[0][1]\n",
    "FN=cm[1][0]\n",
    "p=TP/(TP+FP)\n",
    "r=TP/(TP+FN)\n",
    "print(\"F1-score\",2*(p*r)/(p+r))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the MLC analized, the part of this project is to evaluate, compare and study some more intracate classifiers. For this, the first step is to analize the frequency response of the signal since it has aloth of very usefull information and according to the studies done previously, it shows that this domain is crutial to making a good classifier.\n",
    "\n",
    "This next section is dedicated to analizing the FFT of the desired signal. This will have both the FFT built in feature of numpy and also the FFT done by the approxFFT for comparation of this algorith.\n",
    "\n",
    "Additionaly, the peaks are calculated as well and compared with the librosa algorihm.\n",
    "\n",
    "Its important to note that the max frequency is the nyquist frequency which is half of the sampling rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sampling rate\n",
    "sr = 6666\n",
    "\n",
    "file_name=\"state20\"\n",
    "sample_view=1\n",
    "axis=1\n",
    "peaks_to_show=3\n",
    "\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\corrected_new_motor\\Features')\n",
    "feature_file=open(file_name+\"_Feature.txt\",'r')\n",
    "result_data_raw=feature_file.readlines()\n",
    "#these are all of the features done by the aproxFFT algorithm\n",
    "FFT_Dataset,peaks,peak_amp,mean_std=process_features(result_data_raw,-1)\n",
    "\n",
    "# sampling interval\n",
    "ts = 1.0/sr\n",
    "t = np.arange(0,1,ts)\n",
    "N = len(FFT_Dataset[0][sample_view])\n",
    "n = np.arange(N)\n",
    "T = N/sr\n",
    "freq = n/(T) \n",
    "\n",
    "X=FFT_Dataset[0][sample_view][:len(FFT_Dataset[0][sample_view])//2]\n",
    "Y=FFT_Dataset[1][sample_view][:len(FFT_Dataset[1][sample_view])//2]\n",
    "Z=FFT_Dataset[2][sample_view][:len(FFT_Dataset[2][sample_view])//2]\n",
    "\n",
    "\n",
    "#now the numpy fft comparation\n",
    "\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\corrected_new_motor')\n",
    "feature_file=open(file_name+\".txt\",'r')\n",
    "result_data_raw=feature_file.readlines()\n",
    "float_list=process_raw(result_data_raw)\n",
    "raw_Dataset=np.zeros((3),dtype=object)\n",
    "time=[x/26 for x in range(len(float_list[0]))]\n",
    "for i in range(3):\n",
    "    raw_Dataset[i]=[]\n",
    "i=0\n",
    "x_raw=[]\n",
    "y_raw=[]\n",
    "z_raw=[]\n",
    "print_x=[]\n",
    "print_y=[]\n",
    "print_z=[]\n",
    "for line in float_list[0]:\n",
    "    print_x.append(line[0])\n",
    "    print_y.append(line[1])\n",
    "    print_z.append(line[2])\n",
    "    if(i<N):\n",
    "        x_raw.append(line[0]+15)\n",
    "        y_raw.append(line[1]+1000)\n",
    "        z_raw.append(line[2]+105)\n",
    "        i+=1\n",
    "    else:\n",
    "        raw_Dataset[0].append(x_raw)\n",
    "        raw_Dataset[1].append(y_raw)\n",
    "        raw_Dataset[2].append(z_raw)\n",
    "        i=0\n",
    "        x_raw=[]\n",
    "        y_raw=[]\n",
    "        z_raw=[]    \n",
    "\n",
    "print(len(x_raw))\n",
    "print_x=print_x[:1000]\n",
    "print_y=print_y[:1000]\n",
    "print_z=print_z[:1000]\n",
    "time=time[:1000]\n",
    "fig=plt.figure(figsize=(15, 6))\n",
    "ax1=fig.add_subplot(1, 3, 1)\n",
    "ax1.plot(time,print_x)\n",
    "ax1.legend()\n",
    "ax1.set_xlabel(\"Time[s]\")\n",
    "ax1.set_ylabel(\"X axis[mg]\")\n",
    "ax2=fig.add_subplot(1, 3, 2)\n",
    "ax2.plot(time,print_y)\n",
    "ax2.legend()\n",
    "ax2.set_xlabel(\"Time[s]\")\n",
    "ax2.set_ylabel(\"Y axis[mg]\")\n",
    "ax3=fig.add_subplot(1, 3, 3)\n",
    "ax3.plot(time,print_z)\n",
    "ax3.legend()\n",
    "ax3.set_xlabel(\"Time[s]\")\n",
    "ax3.set_ylabel(\"Z axis[mg]\")\n",
    "fig.suptitle(\"State 1\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "window=np.hamming(N)\n",
    "X_np=fft(raw_Dataset[0][sample_view]*window)\n",
    "Y_np=fft(raw_Dataset[1][sample_view]*window)\n",
    "Z_np=fft(raw_Dataset[2][sample_view]*window)\n",
    "X_np[0]=0\n",
    "Y_np[0]=0\n",
    "Z_np[0]=0\n",
    "freq_np=fftfreq(len(raw_Dataset[0][sample_view]),1/sr)\n",
    "freq_np=freq_np[:len(freq_np)//2]\n",
    "fig = make_subplots(rows=3, cols=2,shared_xaxes=True,vertical_spacing=0.08,subplot_titles=(\"Numpy FFT\",\"ApproxFFT\"))\n",
    "fig.add_trace(go.Scatter(x=freq_np, y=abs(X_np)*7.5,name=\"x_axis\"),row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=freq_np, y=abs(Y_np)*7.5,name=\"y_axis\"),row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=freq_np, y=abs(Z_np)*7.5,name=\"z_axis\"),row=3, col=1)\n",
    "fig.update_layout(title_text=\"FFT comparation\")\n",
    "fig.add_trace(go.Scatter(x=freq, y=X,name=\"x_axis\"),row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=freq, y=Y,name=\"y_axis\"),row=2, col=2)\n",
    "fig.add_trace(go.Scatter(x=freq, y=Z,name=\"z_axis\"),row=3, col=2)\n",
    "\n",
    "fig.show()\n",
    "if axis==2:\n",
    "    axis_to_analize=Z\n",
    "elif axis==1:\n",
    "    axis_to_analize=Y\n",
    "else:\n",
    "    axis_to_analize=X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "peaks_comp=librosa.util.peak_pick(np.array(axis_to_analize),pre_max=len(axis_to_analize)//5, post_max=len(axis_to_analize)//5, pre_avg=3, post_avg=5, delta=0.5, wait=0)\n",
    "fig = make_subplots(rows=1, cols=2,shared_xaxes=True,vertical_spacing=0.08,subplot_titles=(\"Peaks from Peakfinder1_peak\",\"Peaks from Zscore_peak\"))\n",
    "fig.add_trace(go.Scatter(x=freq, y=axis_to_analize,name=\"FFT\"),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=freq, y=axis_to_analize,name=\"FFT\"),row=1,col=2)\n",
    "i=0\n",
    "for vals in peaks[axis][sample_view]:\n",
    "    if(vals>0 and i<6):\n",
    "        fig.add_vline(x=vals, line_width=3, line_dash=\"dash\", line_color=\"green\",row=1,col=1)\n",
    "        i+=1\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\test\\Features')\n",
    "feature_file=open(file_name+\"_Feature.txt\",'r')\n",
    "result_data_raw=feature_file.readlines()\n",
    "#these are all of the features done by the aproxFFT algorithm\n",
    "FFT_Dataset,peaks,peak_amp,mean_std=process_features(result_data_raw,-1)\n",
    "i=0\n",
    "for vals in peaks[axis][sample_view]:\n",
    "    if(vals>0 and i<6):\n",
    "        fig.add_vline(x=vals, line_width=3, line_dash=\"dash\", line_color=\"green\",row=1,col=2)\n",
    "        i+=1\n",
    "fig.show(\"notebook\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections are dedicated to utilizing ML algorithms to attempt to classify the dataset. The following algorithms are utilized:\n",
    "* Kmeans\n",
    "* Decision tree\n",
    "* Random Forest\n",
    "* Auto encoder\n",
    "* SVM\n",
    "* PCA\n",
    "\n",
    "The Datasets utlized have 3 features available from their feature files:\n",
    "* Complete FFT\n",
    "* Peak values\n",
    "* Peak amplitudes\n",
    "\n",
    "These features are organized via the \"build_dataset\" function that returns these 3 features in seperate matrix. They are organized in the following way:\n",
    "\n",
    "class -> axis -> feature\n",
    "\n",
    "Each section has at the begining most of the inputs required for choosing features/defining parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to Random Forest. This is an algorithm that requires little preprocessing and is very efficient on the edge device itself, both in memory and execution time.\n",
    "\n",
    "This algorithm is particularly good at detecting the most important features which can be a great insight for both its utilization and of other algorithms.\n",
    "\n",
    "Sklearn page:https://scikit-learn.org/stable/modules/ensemble.html#forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "#os.chdir(r'..\\Data\\CNC\\Features')\n",
    "os.chdir(r'..\\Data\\corrected_new_motor\\Features')\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "import plotly.express as px\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from micromlgen import port\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#This section has all of the configuration of the kmeans. peaks_or_fft define which feature will be used:0->FFT,1->peaks\n",
    "#n_of_peaks_to_use defines the number of peaks to use if the samples are peaks.\n",
    "#dataset_ration defines the size(%) of the test set.\n",
    "\n",
    "peaks_or_fft=1\n",
    "dataset_ratio=0.33\n",
    "n_of_peaks_to_use=6\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=0\n",
    "add_mean=0\n",
    "add_std=0\n",
    "n_of_samples_per_class=2000\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "mean_std=np.zeros((len(all_files),3),dtype=object)\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n],mean_std[class_n]=process_features(result_data_raw,n_of_samples_per_class)\n",
    "\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=5)\n",
    "clf.fit(X_train,y_train)\n",
    "result = permutation_importance(\n",
    "    clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "print(result.importances_mean)\n",
    "print(X_train[0])\n",
    "\"\"\"\n",
    "feature_names=[\"x_1\",\"x_1_amp\",\"x1_amp_scaled\",\"x_2\",\"x_2_amp\",\"x2_amp_scaled\",\"x_3\",\"x_3_amp\",\"x3_amp_scaled\",\"x_energy\",\"x_std\",\"y_1\",\"y_1_amp\",\"y1_amp_scaled\",\"y_2\",\"y_2_amp\",\"y2_amp_scaled\",\"y_3\",\"y_3_amp\",\"y3_amp_scaled\",\"y_energy\",\"y_std\",\"z_1\",\"z_1_amp\",\"z1_amp_scaled\",\"z_2\",\"z_2_amp\",\"z2_amp_scaled\",\"z_3\",\"z_3_amp\",\"z3_amp_scaled\",\"z_energy\",\"z_std\"]\n",
    "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "N=3\n",
    "res = sorted(range(len(forest_importances)), key = lambda sub: forest_importances[sub])[-N:]\n",
    "\"\"\"\n",
    "test_labels=clf.predict(X_test)\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "DT=clf.estimators_[0]\n",
    "print(clf.estimators_[0].get_depth())\n",
    "print(clf.estimators_[0].get_n_leaves())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, test_labels))\n",
    "print(\"Precision:\",precision_score(y_test, test_labels,average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test, test_labels,average='macro'))\n",
    "print(\"F1 score:\",f1_score(y_test, test_labels,average='macro'))\n",
    "disp.plot()\n",
    "plt.show()\n",
    "os.chdir(r'..\\Models')\n",
    "with open(\"RF_model.h\",'w') as file:\n",
    "    file.write(port(clf))\n",
    "    file.close()\n",
    "max_peaks=[0,0,0]\n",
    "for label in peaks:\n",
    "    for axis_idx,axis in enumerate(label):\n",
    "        for sample in axis:\n",
    "            for i in range(n_of_peaks_to_use):\n",
    "                if sample[i]>max_peaks[axis_idx]:\n",
    "                    max_peaks[axis_idx]=sample[i]\n",
    "max_score=0\n",
    "max_score_cpp=0\n",
    "ccp_alphas=np.arange(0, 0.04, 0.001)\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = RandomForestClassifier(n_estimators=5,random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    test_labels=clf.predict(X_test)\n",
    "    score=clf.score(X_test,y_test)\n",
    "    if score>max_score:\n",
    "        max_score=score\n",
    "        max_score_cpp=ccp_alpha\n",
    "if max_score_cpp==0:\n",
    "    max_score_cpp=0.003\n",
    "clf = RandomForestClassifier(n_estimators=5,random_state=0, ccp_alpha=max_score_cpp)\n",
    "clf.fit(X_train, y_train)\n",
    "test_labels=clf.predict(X_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, test_labels))\n",
    "print(\"Precision:\",precision_score(y_test, test_labels,average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test, test_labels,average='macro'))\n",
    "print(\"F1 score:\",f1_score(y_test, test_labels,average='macro'))\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "with open(\"RF_model_P.h\",'w') as file:\n",
    "    file.write(port(clf))\n",
    "    file.close()\n",
    "\n",
    "x_axis=[]\n",
    "y_axis=[]\n",
    "z_axis=[]\n",
    "labels=[]\n",
    "unique,count=np.unique(y_train,return_counts=True)\n",
    "n_of_training_samples=count[0]\n",
    "print(len(X_train[0]))\n",
    "for j in range(len(all_files)):\n",
    "    for i in range(100):\n",
    "        x_axis.append(X_train[i+j*n_of_training_samples][0])\n",
    "        y_axis.append(X_train[i+j*n_of_training_samples][1])\n",
    "        z_axis.append(X_train[i+j*n_of_training_samples][2])\n",
    "        labels.append(y_train[i+j*n_of_training_samples])\n",
    "data={'x_axis':x_axis,'y_axis':y_axis,'z_axis':z_axis,'labels':labels}\n",
    "df=pd.DataFrame(data)\n",
    "fig = px.scatter_3d(df, x='x_axis', y='y_axis', z='z_axis',\n",
    "            color='labels')\n",
    "fig.update_layout(height=600, width=600,title_text=\"points of data\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to building the SVM model. It utilized the SVC classifier from sklearn.\n",
    "\n",
    "SVM has proven to give good results, however, from the tests done in this section it has shown to be less efficient both in memory (around 100x bigger) and in execution time. \n",
    "\n",
    "Sklearn page:https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from micromlgen import port\n",
    "import sys\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\corrected_new_motor\\Features')\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score,f1_score,precision_score,recall_score\n",
    "\n",
    "#This section has all of the configuration of the kmeans. peaks_or_fft define which feature will be used:0->FFT,1->peaks\n",
    "#n_of_peaks_to_use defines the number of peaks to use if the samples are peaks.\n",
    "#dataset_ration defines the size(%) of the test set.\n",
    "\n",
    "peaks_or_fft=0\n",
    "dataset_ratio=0.33\n",
    "n_of_peaks_to_use=6\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=0\n",
    "add_mean=0\n",
    "add_std=1\n",
    "n_of_samples_per_class=2000\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "mean_std=np.zeros((len(all_files),3),dtype=object)\n",
    "\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n],mean_std[class_n]=process_features(result_data_raw,n_of_samples_per_class)\n",
    "    \n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "clf=svm.SVC(gamma=1 / (9 * np.var(X_train[0])))\n",
    "clf.fit(X_train,y_train)\n",
    "test_labels=clf.predict(X_test)\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "print(type(clf.support_vectors_[0][0]))\n",
    "print(sys.getsizeof(clf.support_vectors_))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, test_labels))\n",
    "print(\"Precision:\",precision_score(y_test, test_labels,average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test, test_labels,average='macro'))\n",
    "print(\"F1 score:\",f1_score(y_test, test_labels,average='macro'))\n",
    "disp.plot()\n",
    "plt.show()\n",
    "os.chdir(r'..\\Models')\n",
    "with open(\"SVC_model.h\",'w') as file:\n",
    "    file.write(port(clf))\n",
    "    file.close()\n",
    "new_svm=[]\n",
    "for vector_idx,vector in enumerate(clf.support_vectors_):\n",
    "    quantized_vector=[]\n",
    "    for val in vector:\n",
    "        print(type(val))\n",
    "        quantized_vector.append(int(val))\n",
    "    new_svm.append(quantized_vector)\n",
    "test_labels=clf.predict(X_test)\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "clf.support_vectors_=new_svm\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, test_labels))\n",
    "print(\"Precision:\",precision_score(y_test, test_labels,average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test, test_labels,average='macro'))\n",
    "print(\"F1 score:\",f1_score(y_test, test_labels,average='macro'))\n",
    "disp.plot()\n",
    "plt.show()\n",
    "with open(\"SVC_model_Q.h\",'w') as file:\n",
    "    file.write(port(clf))\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to making the Decision tree model. \n",
    "\n",
    "In general, if this section has a performance as good as RF, it is recomended to use this model instead since it is more efficient since RF is a collection of DT therefore it is less efficient. This is by no means a hard rule and both algorithms should be analized before deployment.\n",
    "\n",
    "Sklearn page:https://scikit-learn.org/stable/modules/tree.html#tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\corrected_new_motor\\Features')\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score,f1_score,precision_score,recall_score\n",
    "import plotly.express as px\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from micromlgen import port\n",
    "\n",
    "#This section has all of the configuration of the kmeans. peaks_or_fft define which feature will be used:0->FFT,1->peaks\n",
    "#n_of_peaks_to_use defines the number of peaks to use if the samples are peaks.\n",
    "#dataset_ration defines the size(%) of the test set.\n",
    "\n",
    "peaks_or_fft=1\n",
    "dataset_ratio=0.33\n",
    "n_of_peaks_to_use=6\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=0\n",
    "add_mean=0\n",
    "add_std=1\n",
    "n_of_samples_per_class=-1\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "mean_std=np.zeros((len(all_files),3),dtype=object)\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n],mean_std[class_n]=process_features(result_data_raw,n_of_samples_per_class)\n",
    "\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "\n",
    "clf=DecisionTreeClassifier(random_state=0)\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "print(X_train[0])\n",
    "print(impurities)\n",
    "clf.fit(X_train,y_train)\n",
    "test_labels=clf.predict(X_test)\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, test_labels))\n",
    "print(\"Precision:\",precision_score(y_test, test_labels,average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test, test_labels,average='macro'))\n",
    "print(\"F1 score:\",f1_score(y_test, test_labels,average='macro'))\n",
    "disp.plot()\n",
    "plt.show()\n",
    "os.chdir(r'..\\Models')\n",
    "with open(\"DT_model.h\",'w') as file:\n",
    "    file.write(port(clf))\n",
    "    file.close()\n",
    "max_score=0\n",
    "max_score_cpp=0\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    test_labels=clf.predict(X_test)\n",
    "    score=clf.score(X_test,y_test)\n",
    "    if score>max_score:\n",
    "        max_score=score\n",
    "        max_score_cpp=ccp_alpha\n",
    "clf = DecisionTreeClassifier(random_state=0, ccp_alpha=max_score_cpp)\n",
    "clf.fit(X_train, y_train)\n",
    "test_labels=clf.predict(X_test)\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, test_labels))\n",
    "print(\"Precision:\",precision_score(y_test, test_labels,average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test, test_labels,average='macro'))\n",
    "print(\"F1 score:\",f1_score(y_test, test_labels,average='macro'))\n",
    "disp.plot()\n",
    "plt.show()\n",
    "with open(\"DT_model_P.h\",'w') as file:\n",
    "    file.write(port(clf))\n",
    "    file.close()\n",
    "values=clf.tree_.threshold\n",
    "new_tree_threshold=[]\n",
    "for val in values:\n",
    "    new_val=int(val)\n",
    "    new_tree_threshold.append(new_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to K-means. K-means is a noteworthy algorithm because it is unsupervised and is able to define areas of confidence in which, if a sample is outside of it, it can be considered an anomaly or a new state. This can possibly allow for an on training aproach on the edge device.\n",
    "\n",
    " K-means done directly has poor results generaly but by choosing important features it can have decent results. This sections acts as an almost preliminary section for K-means as this algorithm will be expanded by the addition of others.\n",
    "\n",
    " Sklearn page:https://scikit-learn.org/stable/modules/clustering.html#k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\corrected_new_motor\\Features')\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score,f1_score,precision_score,recall_score\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "#This section has all of the configuration of the kmeans. FFT/PEAKS define what samples will be used. Please assign 1 to the desired and 0 to the other.\n",
    "#n_of_peaks_to_use defines the number of peaks to use if the samples are peaks.\n",
    "#dataset_ration defines the size(%) of the test set.\n",
    "\n",
    "peaks_or_fft=1\n",
    "dataset_ratio=0.33\n",
    "n_of_peaks_to_use=6\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=0\n",
    "add_mean=0\n",
    "add_std=0\n",
    "n_of_samples_per_class=-1\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "mean_std=np.zeros((len(all_files),3),dtype=object)\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n],mean_std[class_n]=process_features(result_data_raw,n_of_samples_per_class)\n",
    "\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "\n",
    "\"\"\"\n",
    "fig=make_subplots(rows=1,cols=3,shared_xaxes=True,vertical_spacing=0.08,subplot_titles=(\"X axis\",\"Y axis\",\"Z axis\"))\n",
    "x_peaks=[]\n",
    "y_peaks=[]\n",
    "z_peaks=[]\n",
    "labels=[]\n",
    "i=0\n",
    "for class_n,all_peaks_in_class in enumerate(peaks):\n",
    "    for peak_index in range(50):\n",
    "        x_peaks.append(peaks[class_n][0][peak_index])\n",
    "        y_peaks.append(peaks[class_n][1][peak_index])\n",
    "        z_peaks.append(peaks[class_n][2][peak_index])\n",
    "        labels.append(class_n)\n",
    "    for vals in range(50):\n",
    "        x_peaks[vals+i]=sorted(x_peaks[vals+i])\n",
    "        y_peaks[vals+i]=sorted(y_peaks[vals+i])\n",
    "        z_peaks[vals+i]=sorted(z_peaks[vals+i])\n",
    "    i+=50\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[item[0] for item in x_peaks],y=[item[1] for item in x_peaks],mode=\"markers+text\",marker=dict(color=labels)),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=[item[0] for item in y_peaks],y=[item[1] for item in y_peaks],mode=\"markers+text\",marker=dict(color=labels)),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=[item[0] for item in z_peaks],y=[item[1] for item in z_peaks],mode=\"markers+text\",marker=dict(color=labels)),row=1,col=3)\n",
    "fig.show(\"notebook\")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "new_X_train=[]\n",
    "for sample in X_train:\n",
    "    new_sample=[sample[0],sample[7],sample[8]]\n",
    "    new_X_train.append(new_sample)\n",
    "#X_train=new_X_train\n",
    "new_X_test=[]\n",
    "for sample in X_test:\n",
    "    new_sample=[sample[0],sample[7],sample[8]]\n",
    "    new_X_test.append(new_sample)\n",
    "#X_test=new_X_test\n",
    "\"\"\"\n",
    "WCSS=[]\n",
    "for i in range(1,30):\n",
    "    clf=KMeans(n_clusters=i,init='k-means++',random_state=0,n_init=\"auto\")\n",
    "    clf.fit(X_train,y_train)\n",
    "    WCSS.append(clf.inertia_)\n",
    "fig=px.scatter(x=range(1,30),y=WCSS)\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Number of clusters\",  # X-axis label\n",
    "    yaxis_title=\"Squared Error\",  # Y-axis label\n",
    "    title=\"Squarred Error depending on number of clusters\",  # Title of the plot (optional)\n",
    "    showlegend=True,\n",
    "    height=500,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "kmeans=KMeans(n_clusters=9,random_state=0,n_init=\"auto\",init='k-means++')\n",
    "kmeans.fit(X_train)\n",
    "print(\"inertia:\",clf.inertia_)\n",
    "test_labels=kmeans.predict(X_test)\n",
    "\n",
    "if(n_of_peaks_to_use==1 and peaks_or_fft==1):\n",
    "    x_axis=[]\n",
    "    y_axis=[]\n",
    "    z_axis=[]\n",
    "    labels=[]\n",
    "    unique,count=np.unique(y_train,return_counts=True)\n",
    "    n_of_training_samples=count[0]\n",
    "    for j in range(len(all_files)):\n",
    "        for i in range(100):\n",
    "            x_axis.append(X_train[i+j*n_of_training_samples][0])\n",
    "            y_axis.append(X_train[i+j*n_of_training_samples][1])\n",
    "            z_axis.append(X_train[i+j*n_of_training_samples][2])\n",
    "            labels.append(y_train[i+j*n_of_training_samples])\n",
    "    data={'x_axis':x_axis,'y_axis':y_axis,'z_axis':z_axis,'labels':labels}\n",
    "    df=pd.DataFrame(data)\n",
    "    fig = px.scatter_3d(df, x='x_axis', y='y_axis', z='z_axis',\n",
    "                color='labels')\n",
    "    fig.update_layout(height=600, width=600,title_text=\"points of data\")\n",
    "    fig.show()\n",
    "    x_axis=[]\n",
    "    y_axis=[]\n",
    "    z_axis=[]\n",
    "    labels=[]\n",
    "    for i in range(len(X_test)):\n",
    "        x_axis.append(X_test[i][0])\n",
    "        y_axis.append(X_test[i][1])\n",
    "        z_axis.append(X_test[i][2])\n",
    "        labels.append(test_labels[i])\n",
    "    data={'x_axis':x_axis,'y_axis':y_axis,'z_axis':z_axis,'labels':labels}\n",
    "    df=pd.DataFrame(data)\n",
    "    fig = px.scatter_3d(df, x='x_axis', y='y_axis', z='z_axis',\n",
    "                color='labels')\n",
    "    fig.update_layout(height=600, width=600,title_text=\"test things\")\n",
    "    fig.show()\n",
    "    x_axis=[]\n",
    "    y_axis=[]\n",
    "    z_axis=[]\n",
    "    labels=[]\n",
    "    for i in range(7):\n",
    "        x_axis.append(kmeans.cluster_centers_[i][0])\n",
    "        y_axis.append(kmeans.cluster_centers_[i][1])\n",
    "        z_axis.append(kmeans.cluster_centers_[i][2])\n",
    "        labels.append(i)\n",
    "    data={'x_axis':x_axis,'y_axis':y_axis,'z_axis':z_axis,'labels':labels}\n",
    "    df=pd.DataFrame(data)\n",
    "    fig = px.scatter_3d(df, x='x_axis', y='y_axis', z='z_axis',color='labels')\n",
    "    fig.update_layout(height=600, width=300,title_text=\"cluster\")\n",
    "    fig.show()\n",
    "\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "print(X_train[0])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "os.chdir(r'..\\Models')\n",
    "port_kmeans_to_C(kmeans,\"Kmeans_Q\",X_train,1)\n",
    "port_kmeans_to_C(kmeans,\"Kmeans\",X_train,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to utilizing PCA+Kmeans. Performing PCA before can help make clusters clearer,making K-means perform better, however, it requires computation time and memory and these should be taken into account.\n",
    "\n",
    "Additionaly, it can be used to better understand relevant features. For example, for the \"actual_new_motor\" dataset, energy is a great feature and greatly impacts the PCA+K-means.\n",
    "\n",
    "Sklearn page:https://scikit-learn.org/stable/modules/decomposition.html#pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score,f1_score,precision_score,recall_score\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from micromlgen import port\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\corrected_new_motor\\Features')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#This section has all of the configuration of the kmeans. FFT/PEAKS define what samples will be used. Please assign 1 to the desired and 0 to the other.\n",
    "#n_of_peaks_to_use defines the number of peaks to use if the samples are peaks.\n",
    "#dataset_ration defines the size(%) of the test set.\n",
    "\n",
    "peaks_or_fft=1\n",
    "dataset_ratio=0.33\n",
    "n_of_peaks_to_use=6\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=0\n",
    "add_mean=0\n",
    "add_std=1\n",
    "n_of_samples_per_class=-1\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "mean_std=np.zeros((len(all_files),3),dtype=object)\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n],mean_std[class_n]=process_features(result_data_raw,n_of_samples_per_class)\n",
    "\n",
    "print(len(FFTs[0][0]))\n",
    "\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "\n",
    "fig=make_subplots(rows=1,cols=3,shared_xaxes=True,vertical_spacing=0.08,subplot_titles=(\"X axis\",\"Y axis\",\"Z axis\"))\n",
    "x_peaks=[]\n",
    "y_peaks=[]\n",
    "z_peaks=[]\n",
    "labels=[]\n",
    "i=0\n",
    "for class_n,all_peaks_in_class in enumerate(peaks):\n",
    "    for peak_index in range(50):\n",
    "        x_peaks.append(peaks[class_n][0][peak_index])\n",
    "        y_peaks.append(peaks[class_n][1][peak_index])\n",
    "        z_peaks.append(peaks[class_n][2][peak_index])\n",
    "        labels.append(class_n)\n",
    "    for vals in range(50):\n",
    "        x_peaks[vals+i]=sorted(x_peaks[vals+i])\n",
    "        y_peaks[vals+i]=sorted(y_peaks[vals+i])\n",
    "        z_peaks[vals+i]=sorted(z_peaks[vals+i])\n",
    "    i+=50\n",
    "os.chdir(r'..\\Models')\n",
    "if(peaks_or_fft==1):\n",
    "    final_dimentionality=3\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_pre_pca = scaler.transform(X_train)\n",
    "    X_test_pre_pca = scaler.transform(X_test)\n",
    "    pca = PCA(n_components=final_dimentionality) #152 for fft, 20 for peaks\n",
    "    \n",
    "    pca.fit(X_train_pre_pca)\n",
    "\n",
    "    for idx,val in enumerate(pca.explained_variance_ratio_.cumsum()):\n",
    "        if val>0.9:\n",
    "            print(\"min pca dim:\",idx)\n",
    "            break\n",
    "    X_train_pca=pca.transform(X_train_pre_pca)\n",
    "    X_test_pca=pca.transform(X_test_pre_pca)\n",
    "    if(final_dimentionality==3):\n",
    "        print(\"hello\")\n",
    "        x_axis=[]\n",
    "        y_axis=[]\n",
    "        z_axis=[]\n",
    "        labels=[]\n",
    "        unique,count=np.unique(y_train,return_counts=True)\n",
    "        n_of_training_samples=count[0]\n",
    "        for j in range(len(all_files)):\n",
    "            for i in range(100):\n",
    "                x_axis.append(X_train_pca[i+j*n_of_training_samples][0])\n",
    "                y_axis.append(X_train_pca[i+j*n_of_training_samples][1])\n",
    "                z_axis.append(X_train_pca[i+j*n_of_training_samples][2])\n",
    "                labels.append(y_train[i+j*n_of_training_samples])\n",
    "        data={'1st_eigenvector':x_axis,'2nd_eigenvector':y_axis,'3rd_eigenvector':z_axis,'labels':labels}\n",
    "        df=pd.DataFrame(data)\n",
    "        fig = px.scatter_3d(df, x='1st_eigenvector', y='2nd_eigenvector', z='3rd_eigenvector',\n",
    "                    color='labels')\n",
    "        fig.update_layout(height=600, width=600,title_text=\"Points after PCA\")\n",
    "        fig.show()\n",
    "    X_train=X_train_pca\n",
    "    X_test=X_test_pca\n",
    "    port_scaller(scaler,\"Scaler_Q\",1)\n",
    "    port_scaller(scaler,\"Scaler\",0)\n",
    "\n",
    "    with open(\"PCA.h\",'w') as file:\n",
    "        file.write(port(pca))\n",
    "\n",
    "WCSS=[]\n",
    "for i in range(1,15):\n",
    "    clf=KMeans(n_clusters=i,init='k-means++',random_state=32)\n",
    "    clf.fit(X_train,y_train)\n",
    "    WCSS.append(clf.inertia_)\n",
    "fig=px.scatter(x=range(1,15),y=WCSS)\n",
    "fig.show()\n",
    "kmeans=KMeans(n_clusters=9,init='k-means++',random_state=0)\n",
    "print(\"Inertia\",clf.inertia_)\n",
    "kmeans.fit(X_train)\n",
    "test_labels=kmeans.predict(X_test)\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[0,1,2,3,4,5,6,7,8,9,-1])\n",
    "disp.plot()\n",
    "port_kmeans_to_C(kmeans,\"Kmeans_Q\",X_train,1)\n",
    "port_kmeans_to_C(kmeans,\"Kmeans\",X_train,0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to autoencoder+Kmeans. Autoencoders, just like PCA, can reduce the dimention of the data and in turn represent it better for clustering, however, it requires fine tunning unlike PCA.\n",
    "\n",
    "From the analises done at the time of this writing, autoencoders seem to have a poor performance and cannot represent the data correctly, however, it can be a great tool if made to work since it, by itself, can detect anomalies/new classes without the need for any computation done on K-means.\n",
    "\n",
    "Keras page:https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\actual_new_motor\\Features')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.express as px\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "#This section has all of the configuration of the kmeans. FFT/PEAKS define what samples will be used. Please assign 1 to the desired and 0 to the other.\n",
    "#n_of_peaks_to_use defines the number of peaks to use if the samples are peaks.\n",
    "#dataset_ration defines the size(%) of the test set.\n",
    "\n",
    "peaks_or_fft=1\n",
    "dataset_ratio=0.33\n",
    "n_of_peaks_to_use=3\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=1\n",
    "\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n]=process_features(result_data_raw,-1)\n",
    "fig=make_subplots(rows=1,cols=3,shared_xaxes=True,vertical_spacing=0.08,subplot_titles=(\"X axis\",\"Y axis\",\"Z axis\"))\n",
    "x_peaks=[]\n",
    "y_peaks=[]\n",
    "z_peaks=[]\n",
    "labels=[]\n",
    "i=0\n",
    "for class_n,all_peaks_in_class in enumerate(peaks):\n",
    "    for peak_index in range(50):\n",
    "        x_peaks.append(peaks[class_n][0][peak_index])\n",
    "        y_peaks.append(peaks[class_n][1][peak_index])\n",
    "        z_peaks.append(peaks[class_n][2][peak_index])\n",
    "        labels.append(class_n)\n",
    "    for vals in range(50):\n",
    "        x_peaks[vals+i]=sorted(x_peaks[vals+i])\n",
    "        y_peaks[vals+i]=sorted(y_peaks[vals+i])\n",
    "        z_peaks[vals+i]=sorted(z_peaks[vals+i])\n",
    "    i+=50\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_pre_ae = scaler.transform(X_train)\n",
    "X_test_pre_ae = scaler.transform(X_test)\n",
    "input_size = (n_of_peaks_to_use+use_of_amps*n_of_peaks_to_use+add_energy)*3\n",
    "encoding_dim= 3\n",
    "input_layer = keras.layers.Input(shape=(input_size,))\n",
    "encoding_layer=keras.layers.Dense(encoding_dim,activation='relu')(input_layer)\n",
    "decoding_layer=keras.layers.Dense(input_size,activation='relu')(encoding_layer)\n",
    "autoencoder=keras.Model(input_layer,decoding_layer)\n",
    "encoder=keras.Model(input_layer,encoding_layer)\n",
    "autoencoder.compile(optimizer='adam',loss='mean_squared_error')\n",
    "X_train_input=np.array([np.array(xi) for xi in X_train_pre_ae])\n",
    "X_test_input=np.array([np.array(xi) for xi in X_test_pre_ae])\n",
    "autoencoder.fit(X_train_input,X_train_input,epochs=100,batch_size=256,validation_data=(X_test_input,X_test_input))\n",
    "X_test_pred=autoencoder.predict(X_test_input, batch_size=32, verbose=\"auto\", steps=None, callbacks=None)\n",
    "X_test_autoencoder=encoder.predict(X_test_input, batch_size=32, verbose=\"auto\", steps=None, callbacks=None)\n",
    "X_train_autoencoder=encoder.predict(X_train_input, batch_size=32, verbose=\"auto\", steps=None, callbacks=None)\n",
    "x_axis=[]\n",
    "y_axis=[]\n",
    "z_axis=[]\n",
    "labels=[]\n",
    "unique,count=np.unique(y_train,return_counts=True)\n",
    "n_of_training_samples=count[0]\n",
    "\n",
    "for j in range(len(all_files)):\n",
    "    for i in range(100):\n",
    "        x_axis.append(X_train_autoencoder[i+j*n_of_training_samples][0])\n",
    "        y_axis.append(X_train_autoencoder[i+j*n_of_training_samples][1])\n",
    "        z_axis.append(X_train_autoencoder[i+j*n_of_training_samples][2])\n",
    "        labels.append(y_train[i+j*n_of_training_samples])\n",
    "data={'x_axis':x_axis,'y_axis':y_axis,'z_axis':z_axis,'labels':labels}\n",
    "df=pd.DataFrame(data)\n",
    "fig = px.scatter_3d(df, x='x_axis', y='y_axis', z='z_axis',\n",
    "            color='labels')\n",
    "fig.update_layout(height=600, width=600,title_text=\"points of data\")\n",
    "fig.show()\n",
    "\n",
    "kmeans=KMeans(n_clusters=len(peaks),random_state=0,n_init=\"auto\")\n",
    "kmeans.fit(X_train_autoencoder)\n",
    "test_labels=kmeans.predict(X_test_autoencoder)\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to isolation trees(IT). Isolation trees are similar to DT in structure, however, instead of attempting to classify the dataset, it will perform outlier detection by making sure non outliers run throught most the the tree while outliers stop very close to the initial node. The logic behind using IT for classification is to provide an unsupervised tree based aproach. All classes will have their own IT and one of them will have a non outlier while all of the other will have that sample classified as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\corrected_new_motor\\Features')\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score,f1_score,precision_score,recall_score\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from isotree import IsolationForest\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import memory_profiler \n",
    "import psutil\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "peaks_or_fft=1\n",
    "dataset_ratio=0.67\n",
    "n_of_peaks_to_use=6\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=0\n",
    "add_mean=0\n",
    "add_std=0\n",
    "n_of_samples_per_class=-1\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "mean_std=np.zeros((len(all_files),3),dtype=object)\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n],mean_std[class_n]=process_features(result_data_raw,n_of_samples_per_class)\n",
    "\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "\n",
    "X_train_IT=[]\n",
    "X_test_IT=[]\n",
    "y_test_IT=[]\n",
    "current_state=-1\n",
    "X_train_aux=[]\n",
    "test=0\n",
    "for sample_n,sample in enumerate(X_train):\n",
    "    if(current_state==-1):\n",
    "        current_state=y_train[sample_n]\n",
    "    if(y_train[sample_n]==current_state):\n",
    "        X_train_aux.append(sample)\n",
    "    else:\n",
    "        X_train_IT.append(X_train_aux)\n",
    "        X_train_aux=[]\n",
    "    current_state=y_train[sample_n]\n",
    "X_train_IT.append(X_train_aux)\n",
    "\n",
    "current_state=-1\n",
    "X_test_aux=[]\n",
    "y_test_aux=[]\n",
    "for sample_n,sample in enumerate(X_test):\n",
    "    if(current_state==-1):\n",
    "        current_state=y_test[sample_n]\n",
    "    if(y_test[sample_n]==current_state):\n",
    "        X_test_aux.append(sample)\n",
    "        y_test_aux.append(y_test[sample_n])\n",
    "    else:\n",
    "        X_test_IT.append(X_test_aux)\n",
    "        y_test_IT.append(y_test_aux)\n",
    "        X_test_aux=[]\n",
    "        y_test_aux=[]\n",
    "    current_state=y_test[sample_n]\n",
    "X_test_IT.append(X_test_aux)\n",
    "y_test_IT.append(y_test_aux)\n",
    "\n",
    "clf=[]\n",
    "test=0\n",
    "base_memory_usage = process.memory_info().rss/(1024 * 1024)\n",
    "for all_state_samples in X_train_IT:\n",
    "    temp_clf=IsolationForest(\n",
    "    ndim=2, ntrees=100,\n",
    "    missing_action=\"fail\"\n",
    ") \n",
    "    temp_clf.fit(np.array(all_state_samples))\n",
    "    clf.append(temp_clf) \n",
    "memory_usage = process.memory_info().rss/(1024 * 1024)\n",
    "print(\"mem\",memory_usage - base_memory_usage)\n",
    "\n",
    "       \n",
    "res=[]\n",
    "corr=0\n",
    "err=0\n",
    "test=[]\n",
    "for state,all_samples_in_state in enumerate(X_test_IT):\n",
    "    for sample_n,sample in enumerate(all_samples_in_state):\n",
    "        final_res=0\n",
    "        scores=[]\n",
    "        min_score=999\n",
    "        for IT_class,ITs in enumerate(clf):\n",
    "            sample_to_pred=np.array(sample)\n",
    "            pred=ITs.predict(sample_to_pred.reshape(1,-1))\n",
    "\n",
    "            scores.append(pred)\n",
    "            if pred<min_score:\n",
    "                final_res=IT_class\n",
    "                min_score=pred\n",
    "            if IT_class==4 and state==3:\n",
    "                test.append([3,4,pred])\n",
    "            if IT_class==3 and state==3:\n",
    "                test.append([3,3,pred])\n",
    "        if min_score>0.60:\n",
    "            final_res=-1\n",
    "        res.append(final_res)\n",
    "        if final_res==state:\n",
    "            corr+=1\n",
    "        else:\n",
    "            err+=1\n",
    "print(test)\n",
    "print(corr)\n",
    "print(err)\n",
    "cm=confusion_matrix(y_test[:len(res)],res)\n",
    "print(\"Accuracy:\",accuracy_score(y_test[:len(res)],res))\n",
    "print(\"Precision:\",precision_score(y_test[:len(res)],res,average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test[:len(res)],res,average='macro'))\n",
    "print(\"F1 score:\",f1_score(y_test[:len(res)],res,average='macro'))\n",
    "class_titles=[-1,0,1,2,3,4,5,6,7,8]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=class_titles)\n",
    "disp.plot()\n",
    "vals=[]\n",
    "for it in clf:\n",
    "    vals.append(it.predict(np.array(X_train_IT[4][0]).reshape(1,-1)))\n",
    "print(vals)\n",
    "\n",
    "\n",
    "print(X_train_IT[3][0])\n",
    "print(X_train_IT[4][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will make the actual on training IT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\corrected_new_motor\\Features')\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score,f1_score,precision_score,recall_score\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from isotree import IsolationForest\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import sys\n",
    "\n",
    "peaks_or_fft=1\n",
    "dataset_ratio=0.33\n",
    "n_of_peaks_to_use=6\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=1\n",
    "add_mean=0\n",
    "add_std=0\n",
    "n_of_samples_per_class=-1\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "mean_std=np.zeros((len(all_files),3),dtype=object)\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n],mean_std[class_n]=process_features(result_data_raw,n_of_samples_per_class)\n",
    "\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "\n",
    "print(X_train[0])\n",
    "n_of_sample_to_train=50\n",
    "X_train_IT=[]\n",
    "X_test_IT=[]\n",
    "y_test_IT=[]\n",
    "current_state=-1\n",
    "X_train_aux=[]\n",
    "test=0\n",
    "for sample_n,sample in enumerate(X_train):\n",
    "    if(current_state==-1):\n",
    "        current_state=y_train[sample_n]\n",
    "    if(y_train[sample_n]==current_state):\n",
    "        X_train_aux.append(sample)\n",
    "    else:\n",
    "        X_train_IT.append(X_train_aux)\n",
    "        X_train_aux=[]\n",
    "    current_state=y_train[sample_n]\n",
    "X_train_IT.append(X_train_aux)\n",
    "\n",
    "current_state=-1\n",
    "X_test_aux=[]\n",
    "y_test_aux=[]\n",
    "for sample_n,sample in enumerate(X_test):\n",
    "    if(current_state==-1):\n",
    "        current_state=y_test[sample_n]\n",
    "    if(y_test[sample_n]==current_state):\n",
    "        X_test_aux.append(sample)\n",
    "        y_test_aux.append(y_test[sample_n])\n",
    "    else:\n",
    "        X_test_IT.append(X_test_aux)\n",
    "        y_test_IT.append(y_test_aux)\n",
    "        X_test_aux=[]\n",
    "        y_test_aux=[]\n",
    "    current_state=y_test[sample_n]\n",
    "X_test_IT.append(X_test_aux)\n",
    "y_test_IT.append(y_test_aux)\n",
    "\n",
    "res=[]\n",
    "clf=[]\n",
    "max_score=[]\n",
    "threshold=[]\n",
    "switch_state=0\n",
    "sample_to_train=[]\n",
    "sample_to_train_label=[]\n",
    "\n",
    "for state,all_samples_state in enumerate(X_train_IT):\n",
    "    for sample in all_samples_state:\n",
    "        if len(clf)==0:\n",
    "            switch_state+=1\n",
    "            sample_to_train.append(sample)\n",
    "            sample_to_train_label.append(state)\n",
    "        else:\n",
    "            final_res=0\n",
    "            scores=[]\n",
    "            min_score=999\n",
    "            for IT_class,ITs in enumerate(clf):\n",
    "                pred=ITs.predict(np.array(sample).reshape(1,-1))\n",
    "                scores.append(pred)\n",
    "                if pred<min_score:\n",
    "                    final_res=IT_class\n",
    "                    min_score=pred\n",
    "            if min_score>threshold[final_res]:\n",
    "                final_res=-1\n",
    "                switch_state+=1\n",
    "                sample_to_train.append(sample)\n",
    "                sample_to_train_label.append(state)\n",
    "            elif switch_state>0:\n",
    "                switch_state-=1\n",
    "                sample_to_train.pop(0)\n",
    "                sample_to_train_label.pop(0)\n",
    "            res.append(final_res)\n",
    "        if(switch_state==n_of_sample_to_train):\n",
    "            temp_clf=IsolationForest(ndim=2, ntrees=100,\n",
    "    missing_action=\"fail\")\n",
    "            temp_clf.fit(np.array(sample_to_train))\n",
    "            final_train=[]\n",
    "            test=0\n",
    "            for sample in sample_to_train:\n",
    "                val=temp_clf.predict(np.array(sample).reshape(1,-1))\n",
    "                if val<0.6:\n",
    "                    final_train.append(sample)\n",
    "                else:\n",
    "                    test+=1\n",
    "            final_clf=IsolationForest(ndim=2, ntrees=100,\n",
    "    missing_action=\"fail\")\n",
    "            final_clf.fit(np.array(final_train))\n",
    "            max_score_pred=final_clf.predict(np.array(final_train))\n",
    "            max_score_pred.sort()\n",
    "            threshold.append(np.mean(max_score_pred)+1.5*np.std(max_score_pred))\n",
    "            #threshold.append(0.6)\n",
    "            clf.append(final_clf)\n",
    "            print(\"Made new IT for state:\",state)\n",
    "            switch_state=0\n",
    "            sample_to_train=[] \n",
    "            sample_to_train_label=[]\n",
    "print(\"Done training\")\n",
    "res=[]\n",
    "corr=0\n",
    "err=0\n",
    "merge_test=[0]*(len(clf)+1)\n",
    "for state,all_samples_in_state in enumerate(X_test_IT):\n",
    "    for sample_n,sample in enumerate(all_samples_in_state):\n",
    "        final_res=0\n",
    "        scores=[]\n",
    "        min_score=999\n",
    "        prev_res=-1\n",
    "        for IT_class,ITs in enumerate(clf):\n",
    "            pred=ITs.predict(np.array(sample).reshape(1,-1))\n",
    "            scores.append(pred)\n",
    "            if pred<min_score:\n",
    "                prev_res=final_res\n",
    "                final_res=IT_class\n",
    "                min_score=pred\n",
    "        if min_score>threshold[final_res]:\n",
    "            final_res=-1\n",
    "        if state==3:\n",
    "            if final_res!=-1:\n",
    "                merge_test[final_res]+=1\n",
    "                merge_test[scores.index(sorted(scores)[1])]+=1\n",
    "            else:\n",
    "                merge_test[len(clf)]+=1\n",
    "                merge_test[scores.index(sorted(scores)[0])]+=1\n",
    "        if final_res==-1:\n",
    "            res.append(9)\n",
    "        else:\n",
    "            res.append(final_res)\n",
    "        if final_res==state:\n",
    "            corr+=1\n",
    "        else:\n",
    "            err+=1\n",
    "print(corr)\n",
    "print(err)\n",
    "cm=confusion_matrix(y_test[:len(res)],res)\n",
    "print(\"Accuracy:\",corr/(corr+err)*100)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[0,1,2,3,4,5,6,7,8,-1])\n",
    "disp.plot()\n",
    "print(threshold)\n",
    "print(merge_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans+Incremental PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\corrected_new_motor\\Features')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import sys\n",
    "\n",
    "peaks_or_fft=0\n",
    "dataset_ratio=0.33\n",
    "n_of_peaks_to_use=3\n",
    "use_of_amps=1\n",
    "sort=0\n",
    "add_energy=1\n",
    "add_mean=0\n",
    "add_std=1\n",
    "n_of_samples_per_class=-1\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "mean_std=np.zeros((len(all_files),3),dtype=object)\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n],mean_std[class_n]=process_features(result_data_raw,n_of_samples_per_class)\n",
    "\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "\n",
    "X_train_IT=[]\n",
    "X_test_IT=[]\n",
    "y_test_IT=[]\n",
    "current_state=-1\n",
    "X_train_aux=[]\n",
    "test=0\n",
    "for sample_n,sample in enumerate(X_train):\n",
    "    if(current_state==-1):\n",
    "        current_state=y_train[sample_n]\n",
    "    if(y_train[sample_n]==current_state):\n",
    "        X_train_aux.append(sample)\n",
    "    else:\n",
    "        X_train_IT.append(X_train_aux)\n",
    "        X_train_aux=[]\n",
    "    current_state=y_train[sample_n]\n",
    "X_train_IT.append(X_train_aux)\n",
    "\n",
    "current_state=-1\n",
    "X_test_aux=[]\n",
    "y_test_aux=[]\n",
    "for sample_n,sample in enumerate(X_test):\n",
    "    if(current_state==-1):\n",
    "        current_state=y_test[sample_n]\n",
    "    if(y_test[sample_n]==current_state):\n",
    "        X_test_aux.append(sample)\n",
    "        y_test_aux.append(y_test[sample_n])\n",
    "    else:\n",
    "        X_test_IT.append(X_test_aux)\n",
    "        y_test_IT.append(y_test_aux)\n",
    "        X_test_aux=[]\n",
    "        y_test_aux=[]\n",
    "    current_state=y_test[sample_n]\n",
    "X_test_IT.append(X_test_aux)\n",
    "y_test_IT.append(y_test_aux)\n",
    "\n",
    "clusters=[]\n",
    "max_distances=[]\n",
    "n_of_sample_to_train=100\n",
    "\n",
    "\n",
    "ipca=IncrementalPCA(n_components=3)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_IT[2])\n",
    "ipca.partial_fit(np.array(scaler.transform(X_train_IT[2])))\n",
    "kmeans_data=ipca.transform(np.array(scaler.transform(X_train_IT[2])))\n",
    "kmeans=KMeans(n_clusters=1,init='k-means++',random_state=0)\n",
    "kmeans.fit(kmeans_data)\n",
    "new_cluster=[0]*3\n",
    "for sample in kmeans_data:\n",
    "    for feature_n,feature in enumerate(sample):\n",
    "        new_cluster[feature_n]+=feature\n",
    "for feature in range(3):\n",
    "    new_cluster[feature]=new_cluster[feature]/len(kmeans_data)\n",
    "print(new_cluster)\n",
    "max_distance=0\n",
    "distance_log=[]\n",
    "for point in kmeans_data:\n",
    "    distance=0\n",
    "    for feature_n,feature in enumerate(point):\n",
    "        distance=(feature-kmeans.cluster_centers_[0][feature_n])*(feature-kmeans.cluster_centers_[0][feature_n])\n",
    "    distance=math.sqrt(distance)\n",
    "    distance_log.append(distance)\n",
    "distance_log.sort()\n",
    "mean=np.mean(distance_log)\n",
    "std=np.std(distance_log)\n",
    "new_distance_log=[]\n",
    "test=0\n",
    "for distance in distance_log:\n",
    "    if distance<mean+3*std:\n",
    "        new_distance_log.append(distance)\n",
    "    else:\n",
    "        test+=1\n",
    "\n",
    "print(test)\n",
    "print(mean+3*std)\n",
    "max_distance=new_distance_log[-1]\n",
    "test_points=[]\n",
    "for point in kmeans_data:\n",
    "    distance=0\n",
    "    for feature_n,feature in enumerate(point):\n",
    "        distance+=(feature-kmeans.cluster_centers_[0][feature_n])*(feature-kmeans.cluster_centers_[0][feature_n])\n",
    "    distance=math.sqrt(distance)\n",
    "    if distance>max_distance:\n",
    "        test_points.append(point)\n",
    "print(max_distance)\n",
    "kmeans_test=ipca.transform(np.array(scaler.transform(X_train_IT[3])))\n",
    "test_corr=0\n",
    "test_err=0\n",
    "print(distance_log)\n",
    "distance_log=[]\n",
    "for point in kmeans_test:\n",
    "    distance=0\n",
    "    for feature_n,feature in enumerate(point):\n",
    "        distance+=(feature-kmeans.cluster_centers_[0][feature_n])*(feature-kmeans.cluster_centers_[0][feature_n])\n",
    "    distance=math.sqrt(distance)\n",
    "    distance_log.append(distance)\n",
    "    if distance>max_distance:\n",
    "        test_corr+=1\n",
    "    else:\n",
    "        test_err+=1\n",
    "distance_log.sort()\n",
    "print(distance_log)\n",
    "print(test_err)\n",
    "print(test_corr)\n",
    "print(kmeans.cluster_centers_)\n",
    "x_axis=[]\n",
    "y_axis=[]\n",
    "z_axis=[]\n",
    "label=[]\n",
    "for i in range(len(kmeans_data)):\n",
    "    x_axis.append(kmeans_data[i][0])\n",
    "    y_axis.append(kmeans_data[i][1])\n",
    "    z_axis.append(kmeans_data[i][2])\n",
    "    label.append(0)\n",
    "for i in range(len(kmeans_test)):\n",
    "    x_axis.append(kmeans_test[i][0])\n",
    "    y_axis.append(kmeans_test[i][1])\n",
    "    z_axis.append(kmeans_test[i][2])\n",
    "    label.append(1)\n",
    "for i in range(len(test_points)):\n",
    "    x_axis.append(test_points[i][0])\n",
    "    y_axis.append(test_points[i][1])\n",
    "    z_axis.append(test_points[i][2])\n",
    "    label.append(2)\n",
    "data={'x_axis':x_axis,'y_axis':y_axis,'z_axis':z_axis,'labels':label}\n",
    "df=pd.DataFrame(data)\n",
    "fig = px.scatter_3d(df, x='x_axis', y='y_axis', z='z_axis',color='labels')\n",
    "fig.update_layout(height=600, width=600,title_text=\"points of data\")\n",
    "fig.show()\n",
    "\n",
    "old_cluster=ipca.inverse_transform(kmeans.cluster_centers_.reshape(1,-1))\n",
    "\n",
    "\n",
    "ipca.partial_fit(np.array(scaler.transform(X_train_IT[3])))\n",
    "\n",
    "\n",
    "\n",
    "kmeans_pre_data=X_train_IT[2]\n",
    "kmeans_pre_data.extend(X_train_IT[3])\n",
    "\n",
    "new_cluster=ipca.transform(old_cluster)\n",
    "kmeans_data=ipca.transform(np.array(scaler.transform(kmeans_pre_data)))\n",
    "kmeans_show_1=ipca.transform(np.array(scaler.transform(X_train_IT[2])))\n",
    "kmeans_show_2=ipca.transform(np.array(scaler.transform(X_train_IT[3])))\n",
    "\n",
    "kmeans=KMeans(n_clusters=2,init='k-means++',random_state=0)\n",
    "kmeans.fit(kmeans_data)\n",
    "kmeans_test=ipca.transform(np.array(scaler.transform(X_train_IT[4])))\n",
    "x_axis=[]\n",
    "y_axis=[]\n",
    "z_axis=[]\n",
    "label=[]\n",
    "for i in range(len(kmeans_show_1)):\n",
    "    x_axis.append(kmeans_show_1[i][0])\n",
    "    y_axis.append(kmeans_show_1[i][1])\n",
    "    z_axis.append(kmeans_show_1[i][2])\n",
    "    label.append(0)\n",
    "for i in range(len(kmeans_show_2)):\n",
    "    x_axis.append(kmeans_show_2[i][0])\n",
    "    y_axis.append(kmeans_show_2[i][1])\n",
    "    z_axis.append(kmeans_show_2[i][2])\n",
    "    label.append(1)\n",
    "for i in range(len(kmeans_test)):\n",
    "    x_axis.append(kmeans_test[i][0])\n",
    "    y_axis.append(kmeans_test[i][1])\n",
    "    z_axis.append(kmeans_test[i][2])\n",
    "    label.append(2)\n",
    "data={'x_axis':x_axis,'y_axis':y_axis,'z_axis':z_axis,'labels':label}\n",
    "df=pd.DataFrame(data)\n",
    "fig = px.scatter_3d(df, x='x_axis', y='y_axis', z='z_axis',color='labels')\n",
    "fig.update_layout(height=600, width=600,title_text=\"points of data\")\n",
    "fig.show()\n",
    "print(kmeans.cluster_centers_)\n",
    "print(new_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans+PCA on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\actual_new_motor\\Features')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import sys\n",
    "\n",
    "peaks_or_fft=1\n",
    "dataset_ratio=0.33\n",
    "n_of_peaks_to_use=2\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=1\n",
    "add_mean=0\n",
    "add_std=1\n",
    "n_of_samples_per_class=-1\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "mean_std=np.zeros((len(all_files),3),dtype=object)\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n],mean_std[class_n]=process_features(result_data_raw,n_of_samples_per_class)\n",
    "\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "\n",
    "X_train_IT=[]\n",
    "X_test_IT=[]\n",
    "y_test_IT=[]\n",
    "current_state=-1\n",
    "X_train_aux=[]\n",
    "test=0\n",
    "for sample_n,sample in enumerate(X_train):\n",
    "    if(current_state==-1):\n",
    "        current_state=y_train[sample_n]\n",
    "    if(y_train[sample_n]==current_state):\n",
    "        X_train_aux.append(sample)\n",
    "    else:\n",
    "        X_train_IT.append(X_train_aux)\n",
    "        X_train_aux=[]\n",
    "    current_state=y_train[sample_n]\n",
    "X_train_IT.append(X_train_aux)\n",
    "\n",
    "\n",
    "current_state=-1\n",
    "X_test_aux=[]\n",
    "y_test_aux=[]\n",
    "for sample_n,sample in enumerate(X_test):\n",
    "    if(current_state==-1):\n",
    "        current_state=y_test[sample_n]\n",
    "    if(y_test[sample_n]==current_state):\n",
    "        X_test_aux.append(sample)\n",
    "        y_test_aux.append(y_test[sample_n])\n",
    "    else:\n",
    "        X_test_IT.append(X_test_aux)\n",
    "        y_test_IT.append(y_test_aux)\n",
    "        X_test_aux=[]\n",
    "        y_test_aux=[]\n",
    "    current_state=y_test[sample_n]\n",
    "X_test_IT.append(X_test_aux)\n",
    "y_test_IT.append(y_test_aux)\n",
    "\n",
    "\n",
    "clusters=[]\n",
    "temp_new_clusters=[]\n",
    "max_distances=[]\n",
    "samples_to_train=[]\n",
    "permanent_samples_for_train=[]\n",
    "max_distance_sample=[]\n",
    "batch_size=20\n",
    "batch=[]\n",
    "switch_state=0\n",
    "n_of_samples_to_train=50\n",
    "low_dimention=3\n",
    "x_axis=[]\n",
    "y_axis=[]\n",
    "z_axis=[]\n",
    "label=[]\n",
    "\n",
    "\n",
    "ipca=IncrementalPCA(n_components=low_dimention)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "kmeans= KMeans(n_clusters=1,init='k-means++',random_state=0)\n",
    "for state,all_samples_in_state in enumerate(X_train_IT):\n",
    "    for sample in all_samples_in_state:\n",
    "        if len(clusters)==0:\n",
    "            switch_state+=1\n",
    "            samples_to_train.append(sample)\n",
    "        else:\n",
    "            sample_to_pred=scaler.transform(np.array(sample).reshape(1,-1))\n",
    "            sample_to_pred=ipca.transform(sample_to_pred)[0]\n",
    "            min_distance=0\n",
    "            min_distance_flag=1\n",
    "            final_res=-1\n",
    "            for cluster_n,cluster in enumerate(clusters):\n",
    "                distance=0\n",
    "                for feature_n,feature in enumerate(sample_to_pred):\n",
    "                    distance+=(feature-cluster[feature_n])*(feature-cluster[feature_n])\n",
    "                distance=math.sqrt(distance)\n",
    "                if min_distance_flag==1:\n",
    "                    min_distance_flag=0\n",
    "                    min_distance=distance\n",
    "                    final_res=cluster_n\n",
    "                elif min_distance>distance:\n",
    "                    min_distance=distance\n",
    "                    final_res=cluster_n\n",
    "            if min_distance>max_distances[final_res]:\n",
    "                final_res=-1\n",
    "                switch_state+=1\n",
    "                samples_to_train.append(sample)\n",
    "            elif switch_state>0:\n",
    "                switch_state-=1\n",
    "                samples_to_train.pop(0)\n",
    "        if switch_state==n_of_samples_to_train:\n",
    "            temp_permanent_samples_for_train=[]\n",
    "            new_permanent_samples_for_train=[]\n",
    "            if len(clusters)!=0:\n",
    "                for samples_in_clusters in permanent_samples_for_train:\n",
    "                    new_samples=ipca.inverse_transform(samples_in_clusters)\n",
    "                    new_samples=scaler.inverse_transform(new_samples)\n",
    "                    temp_permanent_samples_for_train.append(new_samples)\n",
    "            scaler.partial_fit(samples_to_train)\n",
    "            samples_to_train=scaler.transform(samples_to_train)\n",
    "            ipca.partial_fit(samples_to_train)\n",
    "            samples_to_train=ipca.transform(samples_to_train)\n",
    "            if len(clusters)!=0:\n",
    "                for samples_in_cluster in temp_permanent_samples_for_train:\n",
    "                    new_samples=scaler.transform(samples_in_cluster)\n",
    "                    new_samples=ipca.transform(new_samples)\n",
    "                    new_permanent_samples_for_train.append(new_samples)\n",
    "            permanent_samples_for_train=new_permanent_samples_for_train\n",
    "            permanent_samples_for_train.append(samples_to_train)\n",
    "            kmeans=KMeans(n_clusters=len(permanent_samples_for_train),init='k-means++',random_state=0)\n",
    "            kmeans_sample=[]\n",
    "            for samples in permanent_samples_for_train:\n",
    "                kmeans_sample.extend(samples)\n",
    "            kmeans.fit(kmeans_sample)\n",
    "            clusters=kmeans.cluster_centers_\n",
    "            max_distances=[]\n",
    "            for cluster_n,samples_in_clusters in enumerate(permanent_samples_for_train):\n",
    "                max_distance=0\n",
    "                distance_log=[]\n",
    "                for sample in samples_in_clusters:\n",
    "                    distance=0\n",
    "                    for feature_n,feature in enumerate(sample):\n",
    "                        distance+=(clusters[cluster_n][feature_n]-feature)*(clusters[cluster_n][feature_n]-feature)\n",
    "                    distance=math.sqrt(distance)\n",
    "                    distance_log.append(distance)\n",
    "                distance_log.sort()\n",
    "                mean=np.mean(distance_log)\n",
    "                std=np.std(distance_log)\n",
    "                max_distances.append(10)\n",
    "            print(\"Detected new state:\",state)\n",
    "            switch_state=0\n",
    "            samples_to_train=[]\n",
    "\n",
    "            \n",
    "\n",
    "                \n",
    "\n",
    "corr=0\n",
    "err=0\n",
    "res=[]\n",
    "x_axis=[]\n",
    "y_axis=[]\n",
    "z_axis=[]\n",
    "label=[]\n",
    "\n",
    "for state,all_samples_in_state in enumerate(X_test_IT):\n",
    "    for sample in all_samples_in_state:\n",
    "        min_distance=0\n",
    "        min_distance_flag=1\n",
    "        final_res=0\n",
    "        sample_to_pred=scaler.transform(np.array(sample).reshape(1,-1))\n",
    "        sample_to_pred=ipca.transform(np.array(sample_to_pred))\n",
    "        x_axis.append(sample_to_pred[0][0])\n",
    "        y_axis.append(sample_to_pred[0][1])\n",
    "        z_axis.append(sample_to_pred[0][2])\n",
    "        label.append(state)\n",
    "\n",
    "        #check which cluster is this point closest too\n",
    "        for cluster_n,cluster in enumerate(clusters):\n",
    "            distance=0\n",
    "            for feature_n,feature in enumerate(sample_to_pred[0]):\n",
    "                distance+=(cluster[feature_n]-feature)*(cluster[feature_n]-feature)\n",
    "            distance=math.sqrt(distance)\n",
    "            if min_distance_flag==1:\n",
    "                min_distance_flag=0\n",
    "                min_distance=distance\n",
    "                final_res=cluster_n\n",
    "            if min_distance>distance:\n",
    "                min_distance=distance\n",
    "                final_res=cluster_n\n",
    "\n",
    "        #check if the point is within the range of the closest cluster, if it isint then its either an outlier or a new state\n",
    "\n",
    "        if min_distance>max_distances[final_res]:\n",
    "            final_res=-1\n",
    "\n",
    "        res.append(final_res)\n",
    "        if final_res==state:\n",
    "            corr+=1\n",
    "        else:\n",
    "            err+=1\n",
    "data={'x_axis':x_axis,'y_axis':y_axis,'z_axis':z_axis,'labels':label}\n",
    "df=pd.DataFrame(data)\n",
    "fig = px.scatter_3d(df, x='x_axis', y='y_axis', z='z_axis',color='labels')\n",
    "fig.update_layout(height=600, width=600,title_text=\"points of data\")\n",
    "fig.show()\n",
    "print(corr)\n",
    "print(err)\n",
    "cm=confusion_matrix(y_test[:len(res)],res)\n",
    "print(\"Accuracy:\",corr/(corr+err)*100)\n",
    "print(cm)\n",
    "print(clusters)\n",
    "print(max_distances)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF+PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\actual_new_motor\\Features')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from isotree import IsolationForest\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "import sys\n",
    "\n",
    "peaks_or_fft=0\n",
    "dataset_ratio=0.7\n",
    "n_of_peaks_to_use=3\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=1\n",
    "add_mean=0\n",
    "add_std=1\n",
    "n_of_samples_per_class=-1\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "FFTs=np.zeros((len(all_files),3),dtype=object)\n",
    "peaks=np.zeros((len(all_files),3),dtype=object)\n",
    "peak_amp=np.zeros((len(all_files),3),dtype=object)\n",
    "mean_std=np.zeros((len(all_files),3),dtype=object)\n",
    "for class_n,file in enumerate(all_files):\n",
    "    feature_file=open(file,'r')\n",
    "    result_data_raw=feature_file.readlines()\n",
    "    FFTs[class_n],peaks[class_n],peak_amp[class_n],mean_std[class_n]=process_features(result_data_raw,n_of_samples_per_class)\n",
    "\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "\n",
    "X_train_IT=[]\n",
    "X_test_IT=[]\n",
    "y_test_IT=[]\n",
    "current_state=-1\n",
    "X_train_aux=[]\n",
    "test=0\n",
    "for sample_n,sample in enumerate(X_train):\n",
    "    if(current_state==-1):\n",
    "        current_state=y_train[sample_n]\n",
    "    if(y_train[sample_n]==current_state):\n",
    "        X_train_aux.append(sample)\n",
    "    else:\n",
    "        X_train_IT.append(X_train_aux)\n",
    "        X_train_aux=[]\n",
    "    current_state=y_train[sample_n]\n",
    "X_train_IT.append(X_train_aux)\n",
    "\n",
    "current_state=-1\n",
    "X_test_aux=[]\n",
    "y_test_aux=[]\n",
    "for sample_n,sample in enumerate(X_test):\n",
    "    if(current_state==-1):\n",
    "        current_state=y_test[sample_n]\n",
    "    if(y_test[sample_n]==current_state):\n",
    "        X_test_aux.append(sample)\n",
    "        y_test_aux.append(y_test[sample_n])\n",
    "    else:\n",
    "        X_test_IT.append(X_test_aux)\n",
    "        y_test_IT.append(y_test_aux)\n",
    "        X_test_aux=[]\n",
    "        y_test_aux=[]\n",
    "    current_state=y_test[sample_n]\n",
    "X_test_IT.append(X_test_aux)\n",
    "y_test_IT.append(y_test_aux)\n",
    "\n",
    "\n",
    "clf=[]\n",
    "switch_state=0\n",
    "n_of_samples_to_train=50\n",
    "thresholds=[]\n",
    "samples_to_train=[]\n",
    "permanent_samples_for_train=[]\n",
    "low_dimention=20\n",
    "ipca=IncrementalPCA(n_components=low_dimention)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "for state,samples_in_state in enumerate(X_train_IT):\n",
    "    for sample in samples_in_state:\n",
    "        if len(clf)==0:\n",
    "            switch_state+=1\n",
    "            samples_to_train.append(sample)\n",
    "        else:\n",
    "            new_sample=scaler.transform(np.array(sample).reshape(1,-1))\n",
    "            new_sample=ipca.transform(new_sample)\n",
    "            min_score=0\n",
    "            min_score_flag=1\n",
    "            final_res=-1\n",
    "            for IF_n,IF in enumerate(clf):\n",
    "                pred=IF.predict(new_sample)\n",
    "                if min_score_flag==1:\n",
    "                    min_score=pred\n",
    "                    min_score_flag=0\n",
    "                    final_res=IF_n\n",
    "                elif min_score>pred:\n",
    "                    min_score=pred\n",
    "                    final_res=IF_n\n",
    "            if min_score>thresholds[final_res]:\n",
    "                final_res=-1\n",
    "                switch_state+=1\n",
    "                samples_to_train.append(sample)\n",
    "            elif switch_state>0:\n",
    "                switch_state-=1\n",
    "                samples_to_train.pop(0)\n",
    "            if(state==3):\n",
    "                print(final_res)\n",
    "                print(min_score)\n",
    "        if switch_state==n_of_samples_to_train:\n",
    "            if len(clf)!=0:\n",
    "                temp_perm_samples=[]\n",
    "                for samples_in_class in permanent_samples_for_train:\n",
    "                    old_samples=ipca.inverse_transform(samples_in_class)\n",
    "                    old_samples=scaler.inverse_transform(old_samples)\n",
    "                    temp_perm_samples.append(old_samples)\n",
    "            scaler.partial_fit(samples_to_train)\n",
    "            new_samples_pred=scaler.transform(samples_to_train)\n",
    "            ipca.partial_fit(new_samples_pred)\n",
    "            if len(clf)!=0:\n",
    "                permanent_samples_for_train=[]\n",
    "                for samples_in_class in temp_perm_samples:\n",
    "                    new_samples=scaler.transform(samples_in_class)\n",
    "                    new_samples=ipca.transform(new_samples)\n",
    "                    permanent_samples_for_train.append(new_samples)\n",
    "            new_samples_pred=ipca.transform(new_samples_pred)\n",
    "            permanent_samples_for_train.append(new_samples_pred)\n",
    "            clf=[]\n",
    "            thresholds=[]\n",
    "            for class_n,samples_in_class in  enumerate(permanent_samples_for_train):\n",
    "                temp_clf=IsolationForest(ndim=2, ntrees=100,missing_action=\"fail\")\n",
    "                temp_clf.fit(samples_in_class)\n",
    "                predictions=temp_clf.predict(samples_in_class)\n",
    "                mean=np.mean(predictions)\n",
    "                std=np.std(predictions)\n",
    "                thresholds.append(mean+std)\n",
    "                clf.append(temp_clf)\n",
    "            print(\"Made a new IF for state:\",state)\n",
    "            print(\"Threshold:\",thresholds[-1])\n",
    "            switch_state=0\n",
    "            samples_to_train=[]\n",
    "print(thresholds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section will simply read all of the time series of a folder and reduce their sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sr=6666\n",
    "desired_sr=1666\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\actual_new_motor')\n",
    "all_files=os.listdir(os.getcwd())\n",
    "size_of_time_series=0\n",
    "for file in all_files:\n",
    "    if file[0]=='s':\n",
    "        size_of_time_series+=1\n",
    "time_series=np.zeros((size_of_time_series,3),dtype=object)\n",
    "class_n=0\n",
    "for file in all_files:\n",
    "    first_val=0\n",
    "    if file[0]=='s':\n",
    "        time_series_file=open(file,'r')\n",
    "        time_series_raw=time_series_file.readlines()\n",
    "        single_time_series=[]\n",
    "        single_time_series,med_res_x,med_res_y,med_res_z=process_raw(time_series_raw)\n",
    "        for sample in single_time_series:\n",
    "            for axis in range(3):\n",
    "                if first_val<=2:\n",
    "                    time_series[class_n][axis]=[sample[axis]]\n",
    "                    first_val+=1\n",
    "                else:\n",
    "                    time_series[class_n][axis].append(sample[axis])\n",
    "        class_n+=1\n",
    "os.chdir('Resampled_Data')\n",
    "for class_n,all_axis in enumerate(time_series):\n",
    "    new_time_serie=[]\n",
    "    for axis in all_axis:\n",
    "        resampled_axis=librosa.resample(np.asfarray(axis),orig_sr=source_sr,target_sr=desired_sr)\n",
    "        new_time_serie.append(resampled_axis)\n",
    "    with open(\"state\"+str(class_n)+\".txt\",'w') as file:\n",
    "        for sample_n in range(len(new_time_serie[0])):\n",
    "            string_to_write=str(new_time_serie[0][sample_n])+\" \"+str(new_time_serie[1][sample_n])+\" \"+str(new_time_serie[2][sample_n])+'\\n'\n",
    "            file.write(string_to_write)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next few sections will be the next steps after continouos clustering(CC) training on the esp32. The idea is now that we have the data classified we can train a supervised model and have a better end result since it can be optimized in terms of battery, memory and computational power.\n",
    "\n",
    "So far, the workflow is:\n",
    "\n",
    "-Gather data(both the features of the states and the log of the CC algorithm)\n",
    "\n",
    "-Train a model, most likely a RF since it was the most promissing ML\n",
    "\n",
    "-Gather the log of the final classifier\n",
    "\n",
    "-Compare and justify results.\n",
    "\n",
    "As a side note, since Contious Clustering is very similar to Kmeans, it was refered to as \"Kmeans on training\" for the majority of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is where the data is gathered from the ESP32. It is crutial that the code being run on the ESP is the \"Kmeans_solo\" code since this is the one that sends the features in the correct format. The features will be seperated into 3 files per classes: FFT,std and peak. This solution is not elegant or optimal, however it works therefore i will only optimize it if i have additional time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serial import Serial\n",
    "import time\n",
    "import keyboard\n",
    "\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\temp_log')\n",
    "ser=Serial('COM8',baudrate=115200)\n",
    "stop=time.time()+10000\n",
    "log_number=0\n",
    "log_lines=0\n",
    "class_number=0\n",
    "log_name=\"log\"+str(log_number)+\".txt\"\n",
    "log_file=open(log_name,'w')\n",
    "FFT_files=[open(\"FFT{}.txt\".format(i),'w') for i in range(20)]\n",
    "std_files=[open(\"std{}.txt\".format(i),'w') for i in range(20)]\n",
    "peak_files=[open(\"peak{}.txt\".format(i),'w') for i in range(20)]\n",
    "peak_vals=[]\n",
    "peak_amp=[]\n",
    "class_counter=-1\n",
    "print(\"Starting:\")\n",
    "line=\"\"\n",
    "while True:\n",
    "    if keyboard.is_pressed('space'):\n",
    "        break\n",
    "    if ser.in_waiting!=0:\n",
    "        try:\n",
    "            line_binary=ser.read(ser.in_waiting)\n",
    "            line+=line_binary.strip().decode('utf-8')\n",
    "            if line_binary[-1]==10:\n",
    "                if(line[0]=='*'):\n",
    "                    sub_line=line.split(':')\n",
    "                    class_n=int(sub_line[0][1])\n",
    "                    log_file.write(str(class_n))\n",
    "                    log_file.write(\"\\n\")\n",
    "                    for idx,val in enumerate(sub_line[1].split(',')): \n",
    "                        FFT_files[class_n].write(val)\n",
    "                        if idx<len(sub_line[1].split(','))-1:\n",
    "                            FFT_files[class_n].write(',')\n",
    "                    FFT_files[class_n].write(\"\\n\")\n",
    "\n",
    "                    for idx,val in enumerate(sub_line[2].split(',')):\n",
    "                        std_files[class_n].write(val)\n",
    "                        if idx<len(sub_line[2].split(','))-1:\n",
    "                            std_files[class_n].write(',')\n",
    "                    std_files[class_n].write(\"\\n\")\n",
    "\n",
    "                    for idx,val in enumerate(sub_line[3].split(',')):\n",
    "                        if idx%2==0:\n",
    "                            peak_vals.append(val)\n",
    "                        else:\n",
    "                            peak_amp.append(val)\n",
    "                    for idx,val in enumerate(peak_vals):\n",
    "                        peak_files[class_n].write(val)\n",
    "                        if idx<len(peak_vals)-1:\n",
    "                            peak_files[class_n].write(',')\n",
    "                    peak_files[class_n].write(\"\\n\")\n",
    "                    for idx,val in enumerate(peak_amp):\n",
    "                        peak_files[class_n].write(val)\n",
    "                        if idx<len(peak_amp)-1:\n",
    "                            peak_files[class_n].write(',')\n",
    "                    peak_files[class_n].write(\"\\n\")\n",
    "                    peak_vals=[]\n",
    "                    peak_amp=[]\n",
    "\n",
    "                    print(\"Got sample of class:{}\".format(class_n))\n",
    "                    \n",
    "                if(line[0]=='+'):\n",
    "                    for val in line:\n",
    "                        if val!='+':\n",
    "                            log_file.write(val)\n",
    "                    log_file.write(\"\\n\")\n",
    "                    log_lines+=1\n",
    "                line=\"\"\n",
    "        except: \n",
    "            print(\"Something went wrong, please make sure the code on the ESP is the Kmeans_solo.\")\n",
    "log_file.close()\n",
    "for idx in range(len(FFT_files)):\n",
    "    file_name=FFT_files[idx].name\n",
    "    FFT_files[idx].seek(0,os.SEEK_END)\n",
    "    if FFT_files[idx].tell()==0:\n",
    "        FFT_files[idx].close()\n",
    "        os.remove(file_name)\n",
    "    else:\n",
    "        FFT_files[idx].close()\n",
    "for idx in range(len(std_files)):\n",
    "    file_name=std_files[idx].name\n",
    "    std_files[idx].seek(0,os.SEEK_END)\n",
    "    if std_files[idx].tell()==0:\n",
    "        std_files[idx].close()\n",
    "        os.remove(file_name)\n",
    "    else:\n",
    "        std_files[idx].close()\n",
    "for idx in range(len(peak_files)):\n",
    "    file_name=peak_files[idx].name\n",
    "    peak_files[idx].seek(0,os.SEEK_END)\n",
    "    if peak_files[idx].tell()==0:\n",
    "        peak_files[idx].close()\n",
    "        os.remove(file_name)\n",
    "    else:\n",
    "        peak_files[idx].close()\n",
    "ser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is where the final classifier will be built. As it stands, it only trains the RF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\temp_log')\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "import plotly.express as px\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from micromlgen import port\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "n_of_classes=0\n",
    "for file in all_files:\n",
    "    if file[0]=='p':\n",
    "        n_of_classes+=1\n",
    "FFTs=np.zeros((n_of_classes,3),dtype=object)\n",
    "peaks=np.zeros((n_of_classes,3),dtype=object)\n",
    "peak_amp=np.zeros((n_of_classes,3),dtype=object)\n",
    "mean_std=np.zeros((n_of_classes,3),dtype=object)\n",
    "\n",
    "peaks_or_fft=1\n",
    "dataset_ratio=0.33\n",
    "n_of_peaks_to_use=3\n",
    "n_of_peaks_available=3\n",
    "use_of_amps=1\n",
    "sort=1\n",
    "add_energy=0\n",
    "add_mean=0\n",
    "add_std=0\n",
    "n_of_samples_per_class=100\n",
    "\n",
    "for file_n,file in enumerate(all_files):\n",
    "    numbers = [i for i in file.split()[0] if i.isdigit()]\n",
    "    class_n=\"\"\n",
    "    for number in numbers:\n",
    "        class_n+=number\n",
    "    if(class_n!=\"\"):\n",
    "        class_n=int(class_n)\n",
    "        #get FFTs\n",
    "        if file[0]=='F' and peaks_or_fft==0:\n",
    "            class_n=int(file[3])\n",
    "            feature_file=open(file,'r')\n",
    "            result_data_raw=feature_file.readlines()\n",
    "            for idx,line in enumerate(result_data_raw):\n",
    "                if idx<n_of_samples_per_class:\n",
    "                    str_list=list(line.strip().split(\",\"))\n",
    "                    float_list=[]\n",
    "                    for val in str_list:\n",
    "                        if val!='':\n",
    "                            float_list.append(float(val))\n",
    "                    for axis in range(3):\n",
    "                        aux=[]\n",
    "                        if idx==0:\n",
    "                            FFTs[class_n][axis]=[]\n",
    "                        for n_of_FFT in range(256):\n",
    "                            aux.append(float_list[n_of_FFT+axis*256])\n",
    "                        FFTs[class_n][axis].append(aux)\n",
    "            feature_file.close()\n",
    "        #get std (im not getting mean because it tends to be useless)\n",
    "        if file[0]=='s' and add_std:\n",
    "            feature_file=open(file,'r')\n",
    "            result_data_raw=feature_file.readlines()\n",
    "            for idx,line in enumerate(result_data_raw):\n",
    "                if idx<n_of_samples_per_class:\n",
    "                    str_list=list(line.strip().split(\",\"))\n",
    "                    float_list=[]\n",
    "                    for val in str_list:\n",
    "                        if val!='':\n",
    "                            float_list.append(float(val))\n",
    "                    for axis in range(3):\n",
    "                        aux=[]\n",
    "                        if idx==0:\n",
    "                            mean_std[class_n][axis]=[]\n",
    "                        aux.append([0,float_list[axis]])\n",
    "                        mean_std[class_n][axis].append(aux)\n",
    "            feature_file.close()\n",
    "\n",
    "        #get peaks\n",
    "        if file[0]=='p':\n",
    "            feature_file=open(file,'r')\n",
    "            result_data_raw=feature_file.readlines()\n",
    "            for idx,line in enumerate(result_data_raw):\n",
    "                if idx/2<n_of_samples_per_class:\n",
    "                    str_list=list(line.strip().split(\",\"))\n",
    "                    float_list=[]\n",
    "                    for val in str_list:\n",
    "                        if val!='':\n",
    "                            float_list.append(float(val))\n",
    "                    for axis in range(3):\n",
    "                        aux=[]\n",
    "                        if idx==0:\n",
    "                            peaks[class_n][axis]=[]\n",
    "                            peak_amp[class_n][axis]=[]\n",
    "                        for n_peaks in range(n_of_peaks_available):\n",
    "                            aux.append(float_list[n_peaks+n_of_peaks_available*axis])\n",
    "                        if idx%2==0:\n",
    "                            peaks[class_n][axis].append(aux)\n",
    "                        else:\n",
    "                            peak_amp[class_n][axis].append(aux)\n",
    "            print(len(result_data_raw)/2)\n",
    "            feature_file.close()\n",
    "X_train,y_train,X_test,y_test=build_dataset(FFTs,peaks,peak_amp,peaks_or_fft,mean_std,dataset_ratio,use_of_amps,n_of_peaks_to_use,sort,add_energy,add_mean,add_std)\n",
    "final_dimentionality=3\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_pre_pca = scaler.transform(X_train)\n",
    "X_test_pre_pca = scaler.transform(X_test)\n",
    "pca = PCA(n_components=final_dimentionality)\n",
    "pca.fit(X_train_pre_pca)\n",
    "X_train_pca=pca.transform(X_train_pre_pca)\n",
    "X_test_pca=pca.transform(X_test_pre_pca)\n",
    "if(final_dimentionality==3):\n",
    "    x_axis=[]\n",
    "    y_axis=[]\n",
    "    z_axis=[]\n",
    "    labels=[]\n",
    "    unique,count=np.unique(y_train,return_counts=True)\n",
    "    print(count)\n",
    "    c=0\n",
    "    for j in range(len(unique)):\n",
    "        n_of_training_samples=count[j]\n",
    "        print(n_of_training_samples)\n",
    "        for i in range(n_of_training_samples):\n",
    "            x_axis.append(X_train_pca[c][0])\n",
    "            y_axis.append(X_train_pca[c][1])\n",
    "            z_axis.append(X_train_pca[c][2])\n",
    "            labels.append(y_train[c])\n",
    "            c+=1\n",
    "    data={'1st eigenvector':x_axis,'2nd eigenvector':y_axis,'3rd eigenvector':z_axis,'labels':labels}\n",
    "    df=pd.DataFrame(data)\n",
    "    fig = px.scatter_3d(df, x='1st eigenvector', y='2nd eigenvector', z='3rd eigenvector',\n",
    "                color='labels')\n",
    "    fig.update_layout(height=600, width=600,title_text=\"points of data\")\n",
    "    fig.show()\n",
    "clf=RandomForestClassifier(n_estimators=5)\n",
    "clf.fit(X_train,y_train)\n",
    "test_labels=clf.predict(X_test)\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(y_test, test_labels))\n",
    "print(\"Precision:\",precision_score(y_test, test_labels,average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test, test_labels,average='macro'))\n",
    "print(\"F1 score:\",f1_score(y_test, test_labels,average='macro'))\n",
    "print(clf.score(X_test,y_test))\n",
    "os.chdir('Models')\n",
    "with open(\"RF_model.h\",'w') as file:\n",
    "    file.write(port(clf))\n",
    "    #write some variables for the final classifier\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"#define USE_PEAK {} \\n\".format(peaks_or_fft))\n",
    "    file.write(\"#define PEAK_N {} \\n\".format(n_of_peaks_to_use))\n",
    "    file.write(\"#define USE_STD {} \\n\".format(add_std))\n",
    "    file.write(\"#define SORT {} \\n\".format(sort))\n",
    "    file.close()\n",
    "\n",
    "max_score=0\n",
    "max_score_cpp=0\n",
    "ccp_alphas=np.arange(0, 0.1, 0.001)\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = RandomForestClassifier(n_estimators=5,random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    test_labels=clf.predict(X_test)\n",
    "    score=clf.score(X_test,y_test)\n",
    "    if score>max_score:\n",
    "        max_score=score\n",
    "        max_score_cpp=ccp_alpha\n",
    "print(max_score_cpp)\n",
    "if max_score_cpp!=0:\n",
    "    clf = RandomForestClassifier(n_estimators=5,random_state=0, ccp_alpha=max_score_cpp)   \n",
    "else: \n",
    "    clf = RandomForestClassifier(n_estimators=5,random_state=0, ccp_alpha=0.003)\n",
    "clf.fit(X_train, y_train)\n",
    "test_labels=clf.predict(X_test)\n",
    "cm=confusion_matrix(y_test,test_labels)\n",
    "print(clf.score(X_test,y_test))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(y_test, test_labels))\n",
    "print(\"Precision:\",precision_score(y_test, test_labels,average='macro'))\n",
    "print(\"Recall:\",recall_score(y_test, test_labels,average='macro'))\n",
    "print(\"F1 score:\",f1_score(y_test, test_labels,average='macro'))\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "with open(\"RF_model_P.h\",'w') as file:\n",
    "    file.write(port(clf))\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"#define USE_PEAK {} \\n\".format(peaks_or_fft))\n",
    "    file.write(\"#define PEAK_N {} \\n\".format(n_of_peaks_to_use))\n",
    "    file.write(\"#define USE_STD {} \\n\".format(add_std))\n",
    "    file.write(\"#define SORT {} \\n\".format(sort))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to logging the final classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serial import Serial\n",
    "import keyboard\n",
    "\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\temp_log')\n",
    "ser=Serial('COM8',baudrate=115200)\n",
    "log_number=0\n",
    "log_lines=0\n",
    "class_number=0\n",
    "log_name=\"RF_log\"+str(log_number)+\".txt\"\n",
    "log_file=open(log_name,'w')\n",
    "class_counter=-1\n",
    "print(\"Logging:\")\n",
    "line=\"\"\n",
    "while True:\n",
    "    if keyboard.is_pressed('space'):\n",
    "        break\n",
    "    if ser.in_waiting!=0:\n",
    "        try:\n",
    "            line_binary=ser.read(ser.in_waiting)\n",
    "            line+=line_binary.strip().decode('utf-8')\n",
    "            if line_binary[-1]==10: \n",
    "                sub_line=line.split(':')   \n",
    "                if(sub_line[2].isnumeric()):\n",
    "                    for val in  sub_line[2]:\n",
    "                         log_file.write(val)\n",
    "                    log_file.write(\"\\n\")\n",
    "                    log_lines+=1\n",
    "                line=\"\" \n",
    "        except: \n",
    "            print (\"Something went wrong, please make sure the code on the ESP32 is the Final_classifier.\")\n",
    "log_file.close()\n",
    "ser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing special, just to get a grafic of a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\coffe_TB_v2')\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "import plotly.express as px\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from micromlgen import port\n",
    "\n",
    "all_files=os.listdir(os.getcwd())\n",
    "n_of_classes=0\n",
    "for file in all_files:\n",
    "    if file[0]=='R':\n",
    "        n_of_classes+=1\n",
    "n_of_classes-=1\n",
    "y_pred=[]\n",
    "y_gt=[]\n",
    "y_rt=[]\n",
    "for file_n,file in enumerate(all_files):\n",
    "    if file[0]=='R':\n",
    "        numbers = [i for i in file.split()[0] if i.isdigit()]\n",
    "        class_n=\"\"\n",
    "        for number in numbers:\n",
    "            class_n+=number\n",
    "        if(class_n!=\"\"):\n",
    "            class_n=int(class_n)\n",
    "            gt_file=open(file)\n",
    "            gt=gt_file.readlines()\n",
    "            for val in gt:\n",
    "                y_pred.append(int(val))\n",
    "                y_gt.append(class_n)\n",
    "        else:\n",
    "            gt_file=open(file)\n",
    "            gt=gt_file.readlines()\n",
    "            for val in gt:\n",
    "                y_rt.append(int(val))\n",
    "cm=confusion_matrix(y_gt,y_pred)\n",
    "cm=np.array([[33,0,0,0,0],\n",
    "[0,33,0,0,0],\n",
    "[0,0,33,0,0],\n",
    "[0,1,1,28,3],\n",
    "[0,0,2,5,26]])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(y_gt,y_pred))\n",
    "print(\"Precision:\",precision_score(y_gt,y_pred,average='macro'))\n",
    "print(\"Recall:\",recall_score(y_gt,y_pred,average='macro'))\n",
    "print(\"F1 score:\",f1_score(y_gt,y_pred,average='macro'))\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_rt,label=\"Predicted values\")\n",
    "plt.legend()\n",
    "plt.title(\"MLC results\")\n",
    "plt.xlabel(\"Sample number\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.show()\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some plots and results from the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\motor_chapt5')\n",
    "\n",
    "kmeans_file=open('log0.txt','r')\n",
    "RF_file=open('RF_log0.txt','r')\n",
    "kmeans_res=[]\n",
    "RF_res=[]\n",
    "raw_data=kmeans_file.readlines()\n",
    "for line in raw_data:\n",
    "    kmeans_res.append(int(line))\n",
    "raw_data=RF_file.readlines()\n",
    "meta=0\n",
    "curr=0\n",
    "for line in raw_data:\n",
    "    if int(line)!=curr:\n",
    "        meta+=1\n",
    "    elif meta>0:\n",
    "        meta-=1\n",
    "    if meta>0:\n",
    "        curr=int(line)\n",
    "        meta=0\n",
    "    RF_res.append(curr)\n",
    "\n",
    "\n",
    "ts=[i * 0.0477 for i in range(len(RF_res))]\n",
    "GT=[]\n",
    "offset=2-12.8\n",
    "\n",
    "#This whole thing is the ground truth, taken from spectrograms\n",
    "\n",
    "for val in ts:\n",
    "    res=0\n",
    "    if val<2-offset:\n",
    "        res=0\n",
    "    elif val>=2-offset and val<4.9-offset:\n",
    "        res=2\n",
    "    elif val>=4.9-offset and val<6-offset:\n",
    "        res=0\n",
    "    elif val>=6-offset and val<11.3-offset:\n",
    "        res=2\n",
    "    elif val>=11.3-offset and val<26.4-offset:\n",
    "        res=1\n",
    "    elif val>=26.4-offset and val<26.8-offset:\n",
    "        res=0\n",
    "    elif val>=26.8-offset and val<32.5-offset:\n",
    "        res=2\n",
    "    elif val>=32.5-offset and val<33.6-offset:\n",
    "        res=0\n",
    "    elif val>=33.6-offset and val<37.9-offset:\n",
    "        res=2\n",
    "    elif val>=37.9-offset and val<39-offset:\n",
    "        res=0\n",
    "    elif val>=39-offset and val<44.2-offset:\n",
    "        res=2\n",
    "    elif val>=44.2-offset and val<61.8-offset:\n",
    "        res=0\n",
    "    elif val>=61.8-offset and val<81.1-offset:\n",
    "        res=2\n",
    "    elif val>=81.10-offset and val<84.5-offset:\n",
    "        res=0\n",
    "    elif val>=84.5-offset and val<89.8-offset:\n",
    "        res=2\n",
    "    elif val>=89.8-offset and val<93-offset:\n",
    "        res=0\n",
    "    elif val>=93-offset and val<102.5-offset:\n",
    "        res=2\n",
    "    elif val>=102.5-offset and val<105.1-offset:\n",
    "        res=0\n",
    "    elif val>=105.1-offset and val<110.4-offset:\n",
    "        res=2\n",
    "    elif val>=110.4-offset and val<111.6-offset:\n",
    "        res=0\n",
    "    elif val>=111.6-offset and val<116.6-offset:\n",
    "        res=2\n",
    "    elif val>=116.6-offset and val<117.2-offset:\n",
    "        res=0\n",
    "    elif val>=117.2-offset and val<121.7-offset:\n",
    "        res=2\n",
    "    elif val>=121.7-offset and val<122.9-offset:\n",
    "        res=0\n",
    "    elif val>=122.9-offset and val<126.9-offset:\n",
    "        res=2\n",
    "    else:\n",
    "        res=0\n",
    "\n",
    "\n",
    "    GT.append(res)\n",
    "cm=confusion_matrix(GT,RF_res)\n",
    "for i,line in enumerate(cm):\n",
    "    for j,val in enumerate(line):\n",
    "        cm[i][j]=cm[i][j]*4\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(GT,RF_res))\n",
    "print(\"Precision:\",precision_score(GT,RF_res,average='macro'))\n",
    "print(\"Recall:\",recall_score(GT,RF_res,average='macro'))\n",
    "print(\"F1 score:\",f1_score(GT,RF_res,average='macro'))\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "fig = make_subplots(rows=2, cols=1,subplot_titles=(\"Results\", \"Supervised results(Random Forest)\"))\n",
    "x=[i for i in range(len(kmeans_res))]\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x, y=kmeans_res,name=\"Ground truth\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        tickmode='linear',  # Force evenly spaced ticks\n",
    "        dtick=1,           # Set tick interval to 2\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time [s]\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"State\", row=1, col=1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=ts, y=RF_res,name=\"Classifier prediction\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Time [s]\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"State\", row=2, col=1)\n",
    "fig.update_layout(height=600, width=1200, title_text=\"Predictions of both logs\",)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section it to turn the csv file from thingsboard into various txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\temp_log')\n",
    "\n",
    "df=pd.read_csv(\"socem_acc_0.csv\")\n",
    "n_of_files=len(df['result'].unique())\n",
    "for state in range(n_of_files):\n",
    "    state_df=df[df['result'] == state]\n",
    "    with open('peak'+str(state),'w') as f:\n",
    "        for line in range(len(state_df)):\n",
    "            columns_to_write=state_df.iloc[line][2:11]\n",
    "            for idx,val in enumerate(columns_to_write):\n",
    "                f.write(str(val))\n",
    "                if idx<len(columns_to_write)-1:\n",
    "                    f.write(',')\n",
    "            f.write('\\n')\n",
    "            columns_to_write=state_df.iloc[line][11:20]\n",
    "            for idx,val in enumerate(columns_to_write):\n",
    "                f.write(str(val))\n",
    "                if idx<len(columns_to_write)-1:\n",
    "                    f.write(',')\n",
    "            f.write('\\n')\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will initialize all of the necessary functions and variables to get the obersation data from Thingsboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter(indent=4)\n",
    "\n",
    "os.chdir(init_dir)\n",
    "os.chdir(r'..\\Data\\temp_log')\n",
    "\n",
    "def adjust_sensors_delay_time(implementation,device):\n",
    "    \"\"\"\n",
    "    This function returns the delay time for each sensor in order to adjust the data\n",
    "    \"\"\"\n",
    "    seconds_delay = 0\n",
    "    milliseconds_delay = 0\n",
    "    if implementation == \"BLE_ESP32S3_CO2\":\n",
    "        if device == \"acc\":\n",
    "            seconds_delay = 2\n",
    "            milliseconds_delay = 920\n",
    "    if implementation == \"WIFI_ArduinoNanoConnect\":\n",
    "        if device == \"mic\":\n",
    "            seconds_delay = 1\n",
    "            milliseconds_delay = 600\n",
    "\n",
    "    return timedelta(seconds=seconds_delay, milliseconds=milliseconds_delay)\n",
    "\n",
    "def jobs_unix_timestamp(job,implementation,device):\n",
    "    epoch = datetime.utcfromtimestamp(0)\n",
    "    if job == \"CNC_SOCEM\":\n",
    "        startDT = datetime(2024, 10, 21, 8, 40, 0, 0) - timedelta(hours=1) + adjust_sensors_delay_time(implementation,device)\n",
    "        startTS = int((startDT - epoch).total_seconds() * 1000.0)\n",
    "        endDT = datetime(2024, 10, 21, 11, 12, 0, 0) - timedelta(hours=1) + adjust_sensors_delay_time(implementation,device)\n",
    "        endTS = int((endDT - epoch).total_seconds() * 1000.0)\n",
    "    return str(startTS), str(endTS)\n",
    "\n",
    "kmeans_keys = [\n",
    "    'PEAK0','PEAK1','PEAK2','PEAK3','PEAK4','PEAK5','PEAK6','PEAK7','PEAK8'\n",
    "    ,'PEAK_AMP0','PEAK_AMP1','PEAK_AMP2','PEAK_AMP3','PEAK_AMP4','PEAK_AMP5'\n",
    "    ,'PEAK_AMP6','PEAK_AMP7','PEAK_AMP8','result'\n",
    "]\n",
    "\n",
    "RF_keys = [\n",
    "    'result'\n",
    "]\n",
    "\n",
    "def acc_json_csv(data, dataset, implementation, device):\n",
    "    acc_data_df = pd.DataFrame()\n",
    "    first_flag = True\n",
    "    \n",
    "    if dataset == \"kmeans\":\n",
    "        acc_analysis_keys = kmeans_keys\n",
    "    if dataset == \"RF\":\n",
    "        acc_analysis_keys = RF_keys\n",
    "\n",
    "    for analysis_key in acc_analysis_keys:\n",
    "        if first_flag:\n",
    "            acc_data_df = pd.DataFrame(data[analysis_key])\n",
    "            acc_data_df = acc_data_df.rename(columns={'value': analysis_key})\n",
    "            first_flag = False\n",
    "        else:\n",
    "            acc_data_df = acc_data_df.join(pd.DataFrame(data[analysis_key]).set_index('ts'), on='ts')\n",
    "            acc_data_df = acc_data_df.rename(columns={'value': analysis_key})\n",
    "    # change the timestamp to seconds\n",
    "    acc_data_df['ts']= acc_data_df['ts'].astype('datetime64[s]')\n",
    "    return acc_data_df.iloc[::-1]\n",
    "\n",
    "def getToken():\n",
    "    username = 'E@mail.com' #email used to create your ThingsBoard account (your email associated with tennant)\n",
    "    password = 'PASSWORD'\t\t#password of your ThingsBoard account (or your tennant account)\n",
    "    url = 'https://SERVER/api/auth/login'\n",
    "    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n",
    "    loginJSON = {'username': username, 'password': password}\n",
    "    tokenAuthResp = requests.post(url, headers=headers, json=loginJSON).json()\n",
    "    print(tokenAuthResp)\n",
    "    token = tokenAuthResp['token']\n",
    "    return token\n",
    "# dataset CNC SOCEM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to get Kmeans observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = getToken()\n",
    "\n",
    "my_headers = {'X-Authorization':  \"Bearer \" + tkn, \"Content-Type\" : \"application/json\"}\n",
    "#print(my_headers)\n",
    "#url to test connection\n",
    "url = \"http://SERVER/api/plugins/telemetry/DEVICE/TOKEN/keys/timeseries\"\n",
    "\n",
    "response = requests.get(url, headers=my_headers)\n",
    "print(response) , print(response.text)\n",
    "\n",
    "my_headers = {'X-Authorization':  \"Bearer \" + tkn, \"Content-Type\" : \"application/json\"}\n",
    "#print(my_headers)\n",
    "\n",
    "# start & end unix timestamps\n",
    "job_name = \"CNC_SOCEM\"\n",
    "acc_startTS, acc_endTS = jobs_unix_timestamp(job_name,\"BLE_ESP32S3_CO2\",\"acc\")\n",
    "\n",
    "# if the time difference between the start and end timestamps is too big, split the job into multiple requests\n",
    "acc_startTS_list = []\n",
    "acc_endTS_list = []\n",
    "acc_startTS_list.append(acc_startTS)\n",
    "print(\"acc_startTS: \" + acc_startTS + \" acc_endTS: \" + acc_endTS, \"acc_endTS - acc_startTS: \" + str(int(acc_endTS) - int(acc_startTS)))\n",
    "if (int(acc_endTS) - int(acc_startTS)) > 100000000:\n",
    "    acc_endTS_list.append(str(int(acc_startTS) + 100000000))\n",
    "    while int(acc_endTS_list[-1]) < int(acc_endTS):\n",
    "        acc_startTS_list.append(str(int(acc_endTS_list[-1]) + 1))\n",
    "        acc_endTS_list.append(str(int(acc_startTS_list[-1]) + 100000000))\n",
    "    acc_endTS_list[-1] = acc_endTS\n",
    "else:\n",
    "    acc_endTS_list.append(acc_endTS)\n",
    "\n",
    "# split the \n",
    "\n",
    "limit = \"1000000000\"\n",
    "\n",
    "date_filename = \"socem\"\n",
    "\n",
    "# acc\n",
    "acc_devices = [\"TOKEN\"]\n",
    "cnt = 0\n",
    "acc_keys = \"ts,\"\n",
    "\n",
    "for analysis_key in kmeans_keys:\n",
    "    acc_keys += analysis_key + \",\"\n",
    "\n",
    "print(acc_keys)\n",
    "data_for_df = []\n",
    "for dev in acc_devices:\n",
    "    acc_data_df = pd.DataFrame()\n",
    "    for acc_startTS, acc_endTS in zip(acc_startTS_list, acc_endTS_list):\n",
    "        url = \"http://SERVER/api/plugins/telemetry/DEVICE/\" + dev + \"/values/timeseries?keys=\" + acc_keys + \"&startTs=\" + acc_startTS + \"&endTs=\" + acc_endTS + \"&limit=\" + limit\n",
    "        response = requests.get(url, headers=my_headers)\n",
    "        print(\"acc response received\")\n",
    "        init_time=0\n",
    "        data = json.loads(response.text)\n",
    "        for key, items in data.items():\n",
    "            for idx,item in enumerate(items):\n",
    "                # Convert each 'ts' value to a human-readable date\n",
    "                if(idx==0):\n",
    "                    init_time=item['ts']\n",
    "                else:\n",
    "                    item['ts']=init_time-5000*idx\n",
    "                item['ts'] = datetime.fromtimestamp(item['ts'] / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(data)\n",
    "        # if json is empty, skip\n",
    "        if not data:\n",
    "            continue\n",
    "        acc_data_df = pd.concat([acc_data_df, acc_json_csv(data, \"kmeans\", \"BLE_ESP32S3_CO2\", \"acc\")])\n",
    "        print(acc_data_df)\n",
    "    acc_data_df.to_csv(date_filename + \"_acc_\" + str(cnt) + \".csv\")\n",
    "    cnt += 1\n",
    "\n",
    "print(\"acc done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to get RF logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = getToken()\n",
    "\n",
    "my_headers = {'X-Authorization':  \"Bearer \" + tkn, \"Content-Type\" : \"application/json\"}\n",
    "#print(my_headers)\n",
    "#url to test connection\n",
    "url = \"http://SERVER/api/plugins/telemetry/DEVICE/TOKEN/keys/timeseries\"\n",
    "\n",
    "response = requests.get(url, headers=my_headers)\n",
    "print(response) , print(response.text)\n",
    "\n",
    "my_headers = {'X-Authorization':  \"Bearer \" + tkn, \"Content-Type\" : \"application/json\"}\n",
    "#print(my_headers)\n",
    "\n",
    "# start & end unix timestamps\n",
    "job_name = \"CNC_SOCEM\"\n",
    "acc_startTS, acc_endTS = jobs_unix_timestamp(job_name,\"BLE_ESP32S3_CO2\",\"acc\")\n",
    "\n",
    "# if the time difference between the start and end timestamps is too big, split the job into multiple requests\n",
    "acc_startTS_list = []\n",
    "acc_endTS_list = []\n",
    "acc_startTS_list.append(acc_startTS)\n",
    "print(\"acc_startTS: \" + acc_startTS + \" acc_endTS: \" + acc_endTS, \"acc_endTS - acc_startTS: \" + str(int(acc_endTS) - int(acc_startTS)))\n",
    "if (int(acc_endTS) - int(acc_startTS)) > 100000000:\n",
    "    acc_endTS_list.append(str(int(acc_startTS) + 100000000))\n",
    "    while int(acc_endTS_list[-1]) < int(acc_endTS):\n",
    "        acc_startTS_list.append(str(int(acc_endTS_list[-1]) + 1))\n",
    "        acc_endTS_list.append(str(int(acc_startTS_list[-1]) + 100000000))\n",
    "    acc_endTS_list[-1] = acc_endTS\n",
    "else:\n",
    "    acc_endTS_list.append(acc_endTS)\n",
    "\n",
    "# split the \n",
    "\n",
    "limit = \"1000000000\"\n",
    "\n",
    "date_filename = \"socem\"\n",
    "\n",
    "# acc\n",
    "acc_devices = [\"TOKEN\"]\n",
    "cnt = 0\n",
    "acc_keys = \"ts,\"\n",
    "\n",
    "for analysis_key in RF_keys:\n",
    "    acc_keys += analysis_key + \",\"\n",
    "\n",
    "print(acc_keys)\n",
    "data_for_df = []\n",
    "for dev in acc_devices:\n",
    "    acc_data_df = pd.DataFrame()\n",
    "    for acc_startTS, acc_endTS in zip(acc_startTS_list, acc_endTS_list):\n",
    "        url = \"http://SERVER/api/plugins/telemetry/DEVICE/\" + dev + \"/values/timeseries?keys=\" + acc_keys + \"&startTs=\" + acc_startTS + \"&endTs=\" + acc_endTS + \"&limit=\" + limit\n",
    "        response = requests.get(url, headers=my_headers)\n",
    "        print(\"acc response received\")\n",
    "        init_time=0\n",
    "        data = json.loads(response.text)\n",
    "        for key, items in data.items():\n",
    "            for idx,item in enumerate(items):\n",
    "                # Convert each 'ts' value to a human-readable date\n",
    "                if(idx==0):\n",
    "                    init_time=item['ts']\n",
    "                else:\n",
    "                    item['ts']=init_time-5000*idx\n",
    "                item['ts'] = datetime.fromtimestamp(item['ts'] / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(data)\n",
    "        # if json is empty, skip\n",
    "        if not data:\n",
    "            continue\n",
    "        acc_data_df = pd.concat([acc_data_df, acc_json_csv(data, \"RF\", \"BLE_ESP32S3_CO2\", \"acc\")])\n",
    "        print(acc_data_df)\n",
    "    acc_data_df.to_csv(date_filename + \"_acc_\" + str(cnt) + \".csv\")\n",
    "    cnt += 1\n",
    "\n",
    "print(\"acc done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
